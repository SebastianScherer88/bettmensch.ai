{"metadata": {"name": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf", "generate_name": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-", "namespace": "argo", "uid": "ae69b1e3-a235-44d5-8667-bef63fc15821", "resource_version": "11463", "generation": 13, "creation_timestamp": "07/12/2024", "labels": {"bettmensch.ai/pipeline-id": "612226a1-b40f-4f68-92c3-ea8a5d6b3995", "bettmensch.ai/pipeline-name": "pipeline-test-torch-gpu-pipeline-7c4zp", "workflows.argoproj.io/completed": "true", "workflows.argoproj.io/creator": "system-serviceaccount-argo-argo-server", "workflows.argoproj.io/phase": "Succeeded"}, "annotations": {"karpenter.sh/do-not-disrupt": "true", "workflows.argoproj.io/pod-name-format": "v2"}, "managed_fields": [{"manager": "argo", "operation": "Update", "api_version": "argoproj.io/v1alpha1", "time": "07/12/2024", "fields_type": "FieldsV1", "fields_v1": {"f:metadata": {"f:generateName": {}, "f:labels": {".": {}, "f:bettmensch.ai/pipeline-id": {}, "f:bettmensch.ai/pipeline-name": {}, "f:workflows.argoproj.io/creator": {}}}, "f:spec": {}}}, {"manager": "workflow-controller", "operation": "Update", "api_version": "argoproj.io/v1alpha1", "time": "07/12/2024", "fields_type": "FieldsV1", "fields_v1": {"f:metadata": {"f:annotations": {".": {}, "f:karpenter.sh/do-not-disrupt": {}, "f:workflows.argoproj.io/pod-name-format": {}}, "f:labels": {"f:workflows.argoproj.io/completed": {}, "f:workflows.argoproj.io/phase": {}}}, "f:status": {}}}]}, "spec": {"arguments": {"parameters": [{"name": "n_iter", "value": "15"}, {"name": "n_seconds_sleep", "value": "2"}]}, "workflow_template_ref": {"name": "pipeline-test-torch-gpu-pipeline-7c4zp"}}, "status": {"phase": "Succeeded", "started_at": "07/12/2024", "finished_at": "07/12/2024", "progress": "5/5", "nodes": {"pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf": {"id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf", "name": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf", "type": "DAG", "display_name": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf", "template_name": "bettmensch-ai-outer-dag", "template_scope": "local/", "phase": "Succeeded", "started_at": "07/12/2024", "finished_at": "07/12/2024", "progress": "5/5", "resources_duration": {"cpu": 23, "memory": 1644, "nvidia.com/gpu": 190}, "children": ["pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-414716060"], "outbound_nodes": ["pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-947069694", "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-41628430", "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-1368447231"]}, "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-1368447231": {"id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-1368447231", "name": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf.bettmensch-ai-inner-dag.torch-ddp-delete-torch-ddp-service", "type": "Pod", "display_name": "torch-ddp-delete-torch-ddp-service", "template_name": "torch-ddp-delete-torch-ddp-service", "template_scope": "local/", "phase": "Succeeded", "boundary_id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-414716060", "started_at": "07/12/2024", "finished_at": "07/12/2024", "progress": "1/1", "resources_duration": {"cpu": 0, "memory": 0}, "outputs": {"exit_code": "0"}, "host_node_name": "ip-10-0-48-85.us-east-2.compute.internal"}, "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-1861925387": {"id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-1861925387", "name": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf.bettmensch-ai-inner-dag.torch-ddp-0(0)", "type": "Pod", "display_name": "torch-ddp-0(0)", "template_name": "torch-ddp-0", "template_scope": "local/", "phase": "Succeeded", "boundary_id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-414716060", "started_at": "07/12/2024", "finished_at": "07/12/2024", "progress": "1/1", "resources_duration": {"cpu": 11, "memory": 839, "nvidia.com/gpu": 99}, "node_flag": {"retried": true}, "inputs": {"parameters": [{"name": "n_iter", "default": "100", "value": "15"}, {"name": "n_seconds_sleep", "default": "10", "value": "2"}, {"name": "duration", "default": "null", "value": "null"}]}, "outputs": {"parameters": [{"name": "duration", "value": "30", "value_from": {"path": "duration"}}], "exit_code": "0"}, "children": ["pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-2733896051", "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-1368447231"], "host_node_name": "ip-10-0-50-210.us-east-2.compute.internal"}, "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-2020597252": {"id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-2020597252", "name": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf.bettmensch-ai-inner-dag.torch-ddp-create-torch-ddp-service", "type": "Pod", "display_name": "torch-ddp-create-torch-ddp-service", "template_name": "torch-ddp-create-torch-ddp-service", "template_scope": "local/", "phase": "Succeeded", "boundary_id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-414716060", "started_at": "07/12/2024", "finished_at": "07/12/2024", "progress": "1/1", "resources_duration": {"cpu": 0, "memory": 1}, "outputs": {"exit_code": "0"}, "children": ["pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-47634872", "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-4097461059"], "host_node_name": "ip-10-0-49-235.us-east-2.compute.internal"}, "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-2733896051": {"id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-2733896051", "name": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf.bettmensch-ai-inner-dag.show-duration-param-0", "type": "Retry", "display_name": "show-duration-param-0", "template_name": "show-duration-param", "template_scope": "local/", "phase": "Succeeded", "boundary_id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-414716060", "started_at": "07/12/2024", "finished_at": "07/12/2024", "progress": "1/1", "resources_duration": {"cpu": 1, "memory": 27}, "inputs": {"parameters": [{"name": "a", "value": "30"}]}, "outputs": {"exit_code": "0"}, "children": ["pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-947069694"]}, "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-4097461059": {"id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-4097461059", "name": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf.bettmensch-ai-inner-dag.torch-ddp-0-worker-1", "type": "Retry", "display_name": "torch-ddp-0-worker-1", "template_name": "torch-ddp-1", "template_scope": "local/", "phase": "Succeeded", "boundary_id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-414716060", "started_at": "07/12/2024", "finished_at": "07/12/2024", "progress": "1/1", "resources_duration": {"cpu": 11, "memory": 777, "nvidia.com/gpu": 91}, "inputs": {"parameters": [{"name": "n_iter", "default": "100", "value": "15"}, {"name": "n_seconds_sleep", "default": "10", "value": "2"}, {"name": "duration", "default": "null", "value": "null"}]}, "outputs": {"parameters": [{"name": "duration", "value": "30", "value_from": {"path": "duration"}}], "exit_code": "0"}, "children": ["pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-41628430"]}, "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-414716060": {"id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-414716060", "name": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf.bettmensch-ai-inner-dag", "type": "DAG", "display_name": "bettmensch-ai-inner-dag", "template_name": "bettmensch-ai-inner-dag", "template_scope": "local/", "phase": "Succeeded", "boundary_id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf", "started_at": "07/12/2024", "finished_at": "07/12/2024", "progress": "5/5", "resources_duration": {"cpu": 23, "memory": 1644, "nvidia.com/gpu": 190}, "inputs": {"parameters": [{"name": "n_iter", "value": "15"}, {"name": "n_seconds_sleep", "value": "2"}]}, "children": ["pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-2020597252"], "outbound_nodes": ["pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-947069694", "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-41628430", "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-1368447231"]}, "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-41628430": {"id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-41628430", "name": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf.bettmensch-ai-inner-dag.torch-ddp-0-worker-1(0)", "type": "Pod", "display_name": "torch-ddp-0-worker-1(0)", "template_name": "torch-ddp-1", "template_scope": "local/", "phase": "Succeeded", "boundary_id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-414716060", "started_at": "07/12/2024", "finished_at": "07/12/2024", "progress": "1/1", "resources_duration": {"cpu": 11, "memory": 777, "nvidia.com/gpu": 91}, "node_flag": {"retried": true}, "inputs": {"parameters": [{"name": "n_iter", "default": "100", "value": "15"}, {"name": "n_seconds_sleep", "default": "10", "value": "2"}, {"name": "duration", "default": "null", "value": "null"}]}, "outputs": {"parameters": [{"name": "duration", "value": "30", "value_from": {"path": "duration"}}], "exit_code": "0"}, "host_node_name": "ip-10-0-50-218.us-east-2.compute.internal"}, "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-47634872": {"id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-47634872", "name": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf.bettmensch-ai-inner-dag.torch-ddp-0", "type": "Retry", "display_name": "torch-ddp-0", "template_name": "torch-ddp-0", "template_scope": "local/", "phase": "Succeeded", "boundary_id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-414716060", "started_at": "07/12/2024", "finished_at": "07/12/2024", "progress": "3/3", "resources_duration": {"cpu": 12, "memory": 866, "nvidia.com/gpu": 99}, "inputs": {"parameters": [{"name": "n_iter", "default": "100", "value": "15"}, {"name": "n_seconds_sleep", "default": "10", "value": "2"}, {"name": "duration", "default": "null", "value": "null"}]}, "outputs": {"parameters": [{"name": "duration", "value": "30", "value_from": {"path": "duration"}}], "exit_code": "0"}, "children": ["pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-1861925387"]}, "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-947069694": {"id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-947069694", "name": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf.bettmensch-ai-inner-dag.show-duration-param-0(0)", "type": "Pod", "display_name": "show-duration-param-0(0)", "template_name": "show-duration-param", "template_scope": "local/", "phase": "Succeeded", "boundary_id": "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-414716060", "started_at": "07/12/2024", "finished_at": "07/12/2024", "progress": "1/1", "resources_duration": {"cpu": 1, "memory": 27}, "node_flag": {"retried": true}, "inputs": {"parameters": [{"name": "a", "value": "30"}]}, "outputs": {"exit_code": "0"}, "host_node_name": "ip-10-0-49-235.us-east-2.compute.internal"}}, "stored_templates": {"namespaced/pipeline-test-torch-gpu-pipeline-7c4zp/bettmensch-ai-inner-dag": {"name": "bettmensch-ai-inner-dag", "inputs": {"parameters": [{"name": "n_iter"}, {"name": "n_seconds_sleep"}]}, "outputs": {}, "metadata": {}, "dag": {"tasks": [{"name": "torch-ddp-create-torch-ddp-service", "template": "torch-ddp-create-torch-ddp-service", "arguments": {}}, {"name": "torch-ddp-0", "template": "torch-ddp-0", "arguments": {"parameters": [{"name": "n_iter", "value": "{{inputs.parameters.n_iter}}"}, {"name": "n_seconds_sleep", "value": "{{inputs.parameters.n_seconds_sleep}}"}]}, "depends": "torch-ddp-create-torch-ddp-service"}, {"name": "torch-ddp-0-worker-1", "template": "torch-ddp-1", "arguments": {"parameters": [{"name": "n_iter", "value": "{{inputs.parameters.n_iter}}"}, {"name": "n_seconds_sleep", "value": "{{inputs.parameters.n_seconds_sleep}}"}]}, "depends": "torch-ddp-create-torch-ddp-service"}, {"name": "torch-ddp-delete-torch-ddp-service", "template": "torch-ddp-delete-torch-ddp-service", "arguments": {}, "depends": "torch-ddp-0"}, {"name": "show-duration-param-0", "template": "show-duration-param", "arguments": {"parameters": [{"name": "a", "value": "{{tasks.torch-ddp-0.outputs.parameters.duration}}"}]}, "depends": "torch-ddp-0"}]}}, "namespaced/pipeline-test-torch-gpu-pipeline-7c4zp/bettmensch-ai-outer-dag": {"name": "bettmensch-ai-outer-dag", "inputs": {}, "outputs": {}, "metadata": {}, "dag": {"tasks": [{"name": "bettmensch-ai-inner-dag", "template": "bettmensch-ai-inner-dag", "arguments": {"parameters": [{"name": "n_iter", "value": "{{workflow.parameters.n_iter}}"}, {"name": "n_seconds_sleep", "value": "{{workflow.parameters.n_seconds_sleep}}"}]}}]}}, "namespaced/pipeline-test-torch-gpu-pipeline-7c4zp/show-duration-param": {"name": "show-duration-param", "inputs": {"parameters": [{"name": "a"}]}, "outputs": {}, "metadata": {}, "script": {"image": "bettmensch88/bettmensch.ai-standard:3.11-latest", "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: a = json.loads(r'''{{inputs.parameters.a}}''')\nexcept: a = r'''{{inputs.parameters.a}}'''\n\nfrom bettmensch_ai.pipelines.io import InputParameter\n\ndef show_parameter(a: InputParameter) -> None:\n    \"\"\"When decorated with the bettmensch_ai.components.component decorator,\n    implements a bettmensch_ai.Component that prints the values of its\n    InputParameter.\"\"\"\n    print(f'Content of input parameter a is: {a}')\n\nshow_parameter(a)\n", "name": "", "command": ["python"], "resources": {"limits": {"cpu": "100m", "memory": "100Mi"}, "requests": {"cpu": "100m", "memory": "100Mi"}}, "image_pull_policy": "Always"}, "retry_strategy": {"limit": "1", "retry_policy": "OnError"}}, "namespaced/pipeline-test-torch-gpu-pipeline-7c4zp/torch-ddp-0": {"name": "torch-ddp-0", "inputs": {"parameters": [{"name": "n_iter", "default": "100"}, {"name": "n_seconds_sleep", "default": "10"}, {"name": "duration", "default": "null"}]}, "outputs": {"parameters": [{"name": "duration", "value_from": {"path": "duration"}}]}, "metadata": {"labels": {"torch-job": "torch-ddp-0", "torch-node": "0"}}, "script": {"image": "bettmensch88/bettmensch.ai-pytorch:3.11-latest", "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: n_iter = json.loads(r'''{{inputs.parameters.n_iter}}''')\nexcept: n_iter = r'''{{inputs.parameters.n_iter}}'''\ntry: n_seconds_sleep = json.loads(r'''{{inputs.parameters.n_seconds_sleep}}''')\nexcept: n_seconds_sleep = r'''{{inputs.parameters.n_seconds_sleep}}'''\n\nfrom bettmensch_ai.pipelines.io import InputParameter\n\nfrom bettmensch_ai.pipelines.io import OutputParameter\nduration = OutputParameter(\"duration\")\n\ndef tensor_reduce(n_iter: InputParameter=100, n_seconds_sleep: InputParameter=10, duration: OutputParameter=None) -> None:\n    \"\"\"When decorated with the torch_component decorator, implements a\n    bettmensch_ai.TorchComponent that runs a torch DDP across pods and nodes in\n    your K8s cluster.\"\"\"\n    import time\n    from datetime import datetime as dt\n    import GPUtil\n    import torch\n    import torch.distributed as dist\n    from bettmensch_ai.pipelines.component.torch_ddp import LaunchContext\n    has_gpu = torch.cuda.is_available()\n    ddp_context = LaunchContext()\n    print(f'GPU present: {has_gpu}')\n    if has_gpu:\n        dist.init_process_group(backend='nccl')\n    else:\n        dist.init_process_group(backend='gloo')\n    for i in range(1, n_iter + 1):\n        time.sleep(n_seconds_sleep)\n        GPUtil.showUtilization()\n        a = torch.tensor([ddp_context.rank])\n        print(f'{i}/{n_iter}: @{dt.now()}')\n        print(f'{i}/{n_iter}: Backend {dist.get_backend()}')\n        print(f'{i}/{n_iter}: Global world size: {ddp_context.world_size}')\n        print(f'{i}/{n_iter}: Global worker process rank: {ddp_context.rank}')\n        print(f'{i}/{n_iter}: This makes me worker process {ddp_context.rank + 1}/{ddp_context.world_size} globally!')\n        print(f'{i}/{n_iter}: Local rank of worker: {ddp_context.local_rank}')\n        print(f'{i}/{n_iter}: Local world size: {ddp_context.local_world_size}')\n        print(f'{i}/{n_iter}: This makes me worker process {ddp_context.local_rank + 1}/{ddp_context.local_world_size} locally!')\n        print(f'{i}/{n_iter}: Node/pod rank: {ddp_context.group_rank}')\n        if has_gpu:\n            device = torch.device(f'cuda:{ddp_context.local_rank}')\n            device_count = torch.cuda.device_count()\n            print(f'{i}/{n_iter}: GPU count: {device_count}')\n            device_name = torch.cuda.get_device_name(ddp_context.local_rank)\n            print(f'{i}/{n_iter}: GPU name: {device_name}')\n            device_property = torch.cuda.get_device_capability(device)\n            print(f'{i}/{n_iter}: GPU property: {device_property}')\n        else:\n            device = torch.device('cpu')\n        a_placed = a.to(device)\n        print(f'{i}/{n_iter}: Pre-`all_reduce` tensor: {a_placed}')\n        dist.all_reduce(a_placed)\n        print(f'{i}/{n_iter}: Post-`all_reduce` tensor: {a_placed}')\n        print('===================================================')\n    if duration is not None:\n        duration_seconds = n_iter * n_seconds_sleep\n        duration.assign(duration_seconds)\n\nfrom torch.distributed.elastic.multiprocessing.errors import record\n\ntensor_reduce=record(tensor_reduce)\n\nfrom bettmensch_ai.pipelines.component import as_torch_ddp\n\ntorch_ddp_decorator=as_torch_ddp()\n\ntorch_ddp_function=torch_ddp_decorator(tensor_reduce)\n\n\ntorch_ddp_function(n_iter,n_seconds_sleep,duration)", "name": "", "command": ["python"], "ports": [{"container_port": 29200, "name": "ddp", "protocol": "TCP"}], "env": [{"name": "NCCL_DEBUG", "value": "INFO"}, {"name": "bettmensch_ai_torch_ddp_min_nodes", "value": "2"}, {"name": "bettmensch_ai_torch_ddp_max_nodes", "value": "2"}, {"name": "bettmensch_ai_torch_ddp_node_rank", "value": "0"}, {"name": "bettmensch_ai_torch_ddp_nproc_per_node", "value": "1"}, {"name": "bettmensch_ai_torch_ddp_max_restarts", "value": "1"}, {"name": "bettmensch_ai_torch_ddp_start_method", "value": "fork"}, {"name": "bettmensch_ai_torch_ddp_rdzv_backend", "value": "static"}, {"name": "bettmensch_ai_torch_ddp_rdzv_endpoint_url", "value": "torch-ddp-0-{{workflow.uid}}.argo.svc.cluster.local"}, {"name": "bettmensch_ai_torch_ddp_rdzv_endpoint_port", "value": "29200"}, {"name": "bettmensch_ai_torch_ddp_run_id", "value": "1"}, {"name": "bettmensch_ai_torch_ddp_tee", "value": "0"}], "resources": {"limits": {"cpu": "100m", "memory": "700Mi", "nvidia.com/gpu": "1"}, "requests": {"cpu": "100m", "memory": "700Mi", "nvidia.com/gpu": "1"}}, "image_pull_policy": "Always"}, "retry_strategy": {"limit": "1", "retry_policy": "OnError"}, "tolerations": [{"key": "nvidia.com/gpu", "operator": "Exists", "effect": "NoSchedule"}], "pod_spec_patch": "topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector:\n    matchExpressions:\n      - { key: torch-node, operator: In, values: ['0','1','2','3','4','5']}"}, "namespaced/pipeline-test-torch-gpu-pipeline-7c4zp/torch-ddp-1": {"name": "torch-ddp-1", "inputs": {"parameters": [{"name": "n_iter", "default": "100"}, {"name": "n_seconds_sleep", "default": "10"}, {"name": "duration", "default": "null"}]}, "outputs": {"parameters": [{"name": "duration", "value_from": {"path": "duration"}}]}, "metadata": {"labels": {"torch-job": "torch-ddp-0", "torch-node": "1"}}, "script": {"image": "bettmensch88/bettmensch.ai-pytorch:3.11-latest", "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: n_iter = json.loads(r'''{{inputs.parameters.n_iter}}''')\nexcept: n_iter = r'''{{inputs.parameters.n_iter}}'''\ntry: n_seconds_sleep = json.loads(r'''{{inputs.parameters.n_seconds_sleep}}''')\nexcept: n_seconds_sleep = r'''{{inputs.parameters.n_seconds_sleep}}'''\n\nfrom bettmensch_ai.pipelines.io import InputParameter\n\nfrom bettmensch_ai.pipelines.io import OutputParameter\nduration = OutputParameter(\"duration\")\n\ndef tensor_reduce(n_iter: InputParameter=100, n_seconds_sleep: InputParameter=10, duration: OutputParameter=None) -> None:\n    \"\"\"When decorated with the torch_component decorator, implements a\n    bettmensch_ai.TorchComponent that runs a torch DDP across pods and nodes in\n    your K8s cluster.\"\"\"\n    import time\n    from datetime import datetime as dt\n    import GPUtil\n    import torch\n    import torch.distributed as dist\n    from bettmensch_ai.pipelines.component.torch_ddp import LaunchContext\n    has_gpu = torch.cuda.is_available()\n    ddp_context = LaunchContext()\n    print(f'GPU present: {has_gpu}')\n    if has_gpu:\n        dist.init_process_group(backend='nccl')\n    else:\n        dist.init_process_group(backend='gloo')\n    for i in range(1, n_iter + 1):\n        time.sleep(n_seconds_sleep)\n        GPUtil.showUtilization()\n        a = torch.tensor([ddp_context.rank])\n        print(f'{i}/{n_iter}: @{dt.now()}')\n        print(f'{i}/{n_iter}: Backend {dist.get_backend()}')\n        print(f'{i}/{n_iter}: Global world size: {ddp_context.world_size}')\n        print(f'{i}/{n_iter}: Global worker process rank: {ddp_context.rank}')\n        print(f'{i}/{n_iter}: This makes me worker process {ddp_context.rank + 1}/{ddp_context.world_size} globally!')\n        print(f'{i}/{n_iter}: Local rank of worker: {ddp_context.local_rank}')\n        print(f'{i}/{n_iter}: Local world size: {ddp_context.local_world_size}')\n        print(f'{i}/{n_iter}: This makes me worker process {ddp_context.local_rank + 1}/{ddp_context.local_world_size} locally!')\n        print(f'{i}/{n_iter}: Node/pod rank: {ddp_context.group_rank}')\n        if has_gpu:\n            device = torch.device(f'cuda:{ddp_context.local_rank}')\n            device_count = torch.cuda.device_count()\n            print(f'{i}/{n_iter}: GPU count: {device_count}')\n            device_name = torch.cuda.get_device_name(ddp_context.local_rank)\n            print(f'{i}/{n_iter}: GPU name: {device_name}')\n            device_property = torch.cuda.get_device_capability(device)\n            print(f'{i}/{n_iter}: GPU property: {device_property}')\n        else:\n            device = torch.device('cpu')\n        a_placed = a.to(device)\n        print(f'{i}/{n_iter}: Pre-`all_reduce` tensor: {a_placed}')\n        dist.all_reduce(a_placed)\n        print(f'{i}/{n_iter}: Post-`all_reduce` tensor: {a_placed}')\n        print('===================================================')\n    if duration is not None:\n        duration_seconds = n_iter * n_seconds_sleep\n        duration.assign(duration_seconds)\n\nfrom torch.distributed.elastic.multiprocessing.errors import record\n\ntensor_reduce=record(tensor_reduce)\n\nfrom bettmensch_ai.pipelines.component import as_torch_ddp\n\ntorch_ddp_decorator=as_torch_ddp()\n\ntorch_ddp_function=torch_ddp_decorator(tensor_reduce)\n\n\ntorch_ddp_function(n_iter,n_seconds_sleep,duration)", "name": "", "command": ["python"], "env": [{"name": "NCCL_DEBUG", "value": "INFO"}, {"name": "bettmensch_ai_torch_ddp_min_nodes", "value": "2"}, {"name": "bettmensch_ai_torch_ddp_max_nodes", "value": "2"}, {"name": "bettmensch_ai_torch_ddp_node_rank", "value": "1"}, {"name": "bettmensch_ai_torch_ddp_nproc_per_node", "value": "1"}, {"name": "bettmensch_ai_torch_ddp_max_restarts", "value": "1"}, {"name": "bettmensch_ai_torch_ddp_start_method", "value": "fork"}, {"name": "bettmensch_ai_torch_ddp_rdzv_backend", "value": "static"}, {"name": "bettmensch_ai_torch_ddp_rdzv_endpoint_url", "value": "torch-ddp-0-{{workflow.uid}}.argo.svc.cluster.local"}, {"name": "bettmensch_ai_torch_ddp_rdzv_endpoint_port", "value": "29200"}, {"name": "bettmensch_ai_torch_ddp_run_id", "value": "1"}, {"name": "bettmensch_ai_torch_ddp_tee", "value": "0"}], "resources": {"limits": {"cpu": "100m", "memory": "700Mi", "nvidia.com/gpu": "1"}, "requests": {"cpu": "100m", "memory": "700Mi", "nvidia.com/gpu": "1"}}, "image_pull_policy": "Always"}, "retry_strategy": {"limit": "1", "retry_policy": "OnError"}, "tolerations": [{"key": "nvidia.com/gpu", "operator": "Exists", "effect": "NoSchedule"}], "pod_spec_patch": "topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector:\n    matchExpressions:\n      - { key: torch-node, operator: In, values: ['0','1','2','3','4','5']}"}, "namespaced/pipeline-test-torch-gpu-pipeline-7c4zp/torch-ddp-create-torch-ddp-service": {"name": "torch-ddp-create-torch-ddp-service", "inputs": {}, "outputs": {}, "metadata": {}, "resource": {"action": "create", "manifest": "apiVersion: v1\nkind: Service\nmetadata:\n  name: torch-ddp-0-{{workflow.uid}}\n  namespace: argo\n  labels:\n    workflows.argoproj.io/workflow: {{workflow.name}}\n    torch-job: torch-ddp-0\nspec:\n  clusterIP: None  # ClusterIP set to None for headless service.\n  ports:\n  - name: ddp  # Port for torchrun master<->worker node coms.\n    port: 29200\n    targetPort: 29200\n  selector:\n    workflows.argoproj.io/workflow: {{workflow.name}}\n    torch-job: torch-ddp-0\n    torch-node: '0'  # Selector for pods associated with this service.\n"}}, "namespaced/pipeline-test-torch-gpu-pipeline-7c4zp/torch-ddp-delete-torch-ddp-service": {"name": "torch-ddp-delete-torch-ddp-service", "inputs": {}, "outputs": {}, "metadata": {}, "resource": {"action": "delete", "flags": ["service", "--selector", "torch-job=torch-ddp-0,workflows.argoproj.io/workflow={{workflow.name}}", "-n", "argo"]}}}, "conditions": [{"type": "PodRunning", "status": "False"}, {"type": "Completed", "status": "True"}], "resources_duration": {"cpu": 23, "memory": 1644, "nvidia.com/gpu": 190}, "stored_workflow_template_spec": {"templates": [{"name": "torch-ddp-create-torch-ddp-service", "inputs": {}, "outputs": {}, "metadata": {}, "resource": {"action": "create", "manifest": "apiVersion: v1\nkind: Service\nmetadata:\n  name: torch-ddp-0-{{workflow.uid}}\n  namespace: argo\n  labels:\n    workflows.argoproj.io/workflow: {{workflow.name}}\n    torch-job: torch-ddp-0\nspec:\n  clusterIP: None  # ClusterIP set to None for headless service.\n  ports:\n  - name: ddp  # Port for torchrun master<->worker node coms.\n    port: 29200\n    targetPort: 29200\n  selector:\n    workflows.argoproj.io/workflow: {{workflow.name}}\n    torch-job: torch-ddp-0\n    torch-node: '0'  # Selector for pods associated with this service.\n"}}, {"name": "torch-ddp-delete-torch-ddp-service", "inputs": {}, "outputs": {}, "metadata": {}, "resource": {"action": "delete", "flags": ["service", "--selector", "torch-job=torch-ddp-0,workflows.argoproj.io/workflow={{workflow.name}}", "-n", "argo"]}}, {"name": "bettmensch-ai-inner-dag", "inputs": {"parameters": [{"name": "n_iter"}, {"name": "n_seconds_sleep"}]}, "outputs": {}, "metadata": {}, "dag": {"tasks": [{"name": "torch-ddp-create-torch-ddp-service", "template": "torch-ddp-create-torch-ddp-service", "arguments": {}}, {"name": "torch-ddp-0", "template": "torch-ddp-0", "arguments": {"parameters": [{"name": "n_iter", "value": "{{inputs.parameters.n_iter}}"}, {"name": "n_seconds_sleep", "value": "{{inputs.parameters.n_seconds_sleep}}"}]}, "depends": "torch-ddp-create-torch-ddp-service"}, {"name": "torch-ddp-0-worker-1", "template": "torch-ddp-1", "arguments": {"parameters": [{"name": "n_iter", "value": "{{inputs.parameters.n_iter}}"}, {"name": "n_seconds_sleep", "value": "{{inputs.parameters.n_seconds_sleep}}"}]}, "depends": "torch-ddp-create-torch-ddp-service"}, {"name": "torch-ddp-delete-torch-ddp-service", "template": "torch-ddp-delete-torch-ddp-service", "arguments": {}, "depends": "torch-ddp-0"}, {"name": "show-duration-param-0", "template": "show-duration-param", "arguments": {"parameters": [{"name": "a", "value": "{{tasks.torch-ddp-0.outputs.parameters.duration}}"}]}, "depends": "torch-ddp-0"}]}}, {"name": "torch-ddp-0", "inputs": {"parameters": [{"name": "n_iter", "default": "100"}, {"name": "n_seconds_sleep", "default": "10"}, {"name": "duration", "default": "null"}]}, "outputs": {"parameters": [{"name": "duration", "value_from": {"path": "duration"}}]}, "metadata": {"labels": {"torch-job": "torch-ddp-0", "torch-node": "0"}}, "script": {"image": "bettmensch88/bettmensch.ai-pytorch:3.11-latest", "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: n_iter = json.loads(r'''{{inputs.parameters.n_iter}}''')\nexcept: n_iter = r'''{{inputs.parameters.n_iter}}'''\ntry: n_seconds_sleep = json.loads(r'''{{inputs.parameters.n_seconds_sleep}}''')\nexcept: n_seconds_sleep = r'''{{inputs.parameters.n_seconds_sleep}}'''\n\nfrom bettmensch_ai.pipelines.io import InputParameter\n\nfrom bettmensch_ai.pipelines.io import OutputParameter\nduration = OutputParameter(\"duration\")\n\ndef tensor_reduce(n_iter: InputParameter=100, n_seconds_sleep: InputParameter=10, duration: OutputParameter=None) -> None:\n    \"\"\"When decorated with the torch_component decorator, implements a\n    bettmensch_ai.TorchComponent that runs a torch DDP across pods and nodes in\n    your K8s cluster.\"\"\"\n    import time\n    from datetime import datetime as dt\n    import GPUtil\n    import torch\n    import torch.distributed as dist\n    from bettmensch_ai.pipelines.component.torch_ddp import LaunchContext\n    has_gpu = torch.cuda.is_available()\n    ddp_context = LaunchContext()\n    print(f'GPU present: {has_gpu}')\n    if has_gpu:\n        dist.init_process_group(backend='nccl')\n    else:\n        dist.init_process_group(backend='gloo')\n    for i in range(1, n_iter + 1):\n        time.sleep(n_seconds_sleep)\n        GPUtil.showUtilization()\n        a = torch.tensor([ddp_context.rank])\n        print(f'{i}/{n_iter}: @{dt.now()}')\n        print(f'{i}/{n_iter}: Backend {dist.get_backend()}')\n        print(f'{i}/{n_iter}: Global world size: {ddp_context.world_size}')\n        print(f'{i}/{n_iter}: Global worker process rank: {ddp_context.rank}')\n        print(f'{i}/{n_iter}: This makes me worker process {ddp_context.rank + 1}/{ddp_context.world_size} globally!')\n        print(f'{i}/{n_iter}: Local rank of worker: {ddp_context.local_rank}')\n        print(f'{i}/{n_iter}: Local world size: {ddp_context.local_world_size}')\n        print(f'{i}/{n_iter}: This makes me worker process {ddp_context.local_rank + 1}/{ddp_context.local_world_size} locally!')\n        print(f'{i}/{n_iter}: Node/pod rank: {ddp_context.group_rank}')\n        if has_gpu:\n            device = torch.device(f'cuda:{ddp_context.local_rank}')\n            device_count = torch.cuda.device_count()\n            print(f'{i}/{n_iter}: GPU count: {device_count}')\n            device_name = torch.cuda.get_device_name(ddp_context.local_rank)\n            print(f'{i}/{n_iter}: GPU name: {device_name}')\n            device_property = torch.cuda.get_device_capability(device)\n            print(f'{i}/{n_iter}: GPU property: {device_property}')\n        else:\n            device = torch.device('cpu')\n        a_placed = a.to(device)\n        print(f'{i}/{n_iter}: Pre-`all_reduce` tensor: {a_placed}')\n        dist.all_reduce(a_placed)\n        print(f'{i}/{n_iter}: Post-`all_reduce` tensor: {a_placed}')\n        print('===================================================')\n    if duration is not None:\n        duration_seconds = n_iter * n_seconds_sleep\n        duration.assign(duration_seconds)\n\nfrom torch.distributed.elastic.multiprocessing.errors import record\n\ntensor_reduce=record(tensor_reduce)\n\nfrom bettmensch_ai.pipelines.component import as_torch_ddp\n\ntorch_ddp_decorator=as_torch_ddp()\n\ntorch_ddp_function=torch_ddp_decorator(tensor_reduce)\n\n\ntorch_ddp_function(n_iter,n_seconds_sleep,duration)", "name": "", "command": ["python"], "ports": [{"container_port": 29200, "name": "ddp", "protocol": "TCP"}], "env": [{"name": "NCCL_DEBUG", "value": "INFO"}, {"name": "bettmensch_ai_torch_ddp_min_nodes", "value": "2"}, {"name": "bettmensch_ai_torch_ddp_max_nodes", "value": "2"}, {"name": "bettmensch_ai_torch_ddp_node_rank", "value": "0"}, {"name": "bettmensch_ai_torch_ddp_nproc_per_node", "value": "1"}, {"name": "bettmensch_ai_torch_ddp_max_restarts", "value": "1"}, {"name": "bettmensch_ai_torch_ddp_start_method", "value": "fork"}, {"name": "bettmensch_ai_torch_ddp_rdzv_backend", "value": "static"}, {"name": "bettmensch_ai_torch_ddp_rdzv_endpoint_url", "value": "torch-ddp-0-{{workflow.uid}}.argo.svc.cluster.local"}, {"name": "bettmensch_ai_torch_ddp_rdzv_endpoint_port", "value": "29200"}, {"name": "bettmensch_ai_torch_ddp_run_id", "value": "1"}, {"name": "bettmensch_ai_torch_ddp_tee", "value": "0"}], "resources": {"limits": {"cpu": "100m", "memory": "700Mi", "nvidia.com/gpu": "1"}, "requests": {"cpu": "100m", "memory": "700Mi", "nvidia.com/gpu": "1"}}, "image_pull_policy": "Always"}, "retry_strategy": {"limit": "1", "retry_policy": "OnError"}, "tolerations": [{"key": "nvidia.com/gpu", "operator": "Exists", "effect": "NoSchedule"}], "pod_spec_patch": "topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector:\n    matchExpressions:\n      - { key: torch-node, operator: In, values: ['0','1','2','3','4','5']}"}, {"name": "torch-ddp-1", "inputs": {"parameters": [{"name": "n_iter", "default": "100"}, {"name": "n_seconds_sleep", "default": "10"}, {"name": "duration", "default": "null"}]}, "outputs": {"parameters": [{"name": "duration", "value_from": {"path": "duration"}}]}, "metadata": {"labels": {"torch-job": "torch-ddp-0", "torch-node": "1"}}, "script": {"image": "bettmensch88/bettmensch.ai-pytorch:3.11-latest", "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: n_iter = json.loads(r'''{{inputs.parameters.n_iter}}''')\nexcept: n_iter = r'''{{inputs.parameters.n_iter}}'''\ntry: n_seconds_sleep = json.loads(r'''{{inputs.parameters.n_seconds_sleep}}''')\nexcept: n_seconds_sleep = r'''{{inputs.parameters.n_seconds_sleep}}'''\n\nfrom bettmensch_ai.pipelines.io import InputParameter\n\nfrom bettmensch_ai.pipelines.io import OutputParameter\nduration = OutputParameter(\"duration\")\n\ndef tensor_reduce(n_iter: InputParameter=100, n_seconds_sleep: InputParameter=10, duration: OutputParameter=None) -> None:\n    \"\"\"When decorated with the torch_component decorator, implements a\n    bettmensch_ai.TorchComponent that runs a torch DDP across pods and nodes in\n    your K8s cluster.\"\"\"\n    import time\n    from datetime import datetime as dt\n    import GPUtil\n    import torch\n    import torch.distributed as dist\n    from bettmensch_ai.pipelines.component.torch_ddp import LaunchContext\n    has_gpu = torch.cuda.is_available()\n    ddp_context = LaunchContext()\n    print(f'GPU present: {has_gpu}')\n    if has_gpu:\n        dist.init_process_group(backend='nccl')\n    else:\n        dist.init_process_group(backend='gloo')\n    for i in range(1, n_iter + 1):\n        time.sleep(n_seconds_sleep)\n        GPUtil.showUtilization()\n        a = torch.tensor([ddp_context.rank])\n        print(f'{i}/{n_iter}: @{dt.now()}')\n        print(f'{i}/{n_iter}: Backend {dist.get_backend()}')\n        print(f'{i}/{n_iter}: Global world size: {ddp_context.world_size}')\n        print(f'{i}/{n_iter}: Global worker process rank: {ddp_context.rank}')\n        print(f'{i}/{n_iter}: This makes me worker process {ddp_context.rank + 1}/{ddp_context.world_size} globally!')\n        print(f'{i}/{n_iter}: Local rank of worker: {ddp_context.local_rank}')\n        print(f'{i}/{n_iter}: Local world size: {ddp_context.local_world_size}')\n        print(f'{i}/{n_iter}: This makes me worker process {ddp_context.local_rank + 1}/{ddp_context.local_world_size} locally!')\n        print(f'{i}/{n_iter}: Node/pod rank: {ddp_context.group_rank}')\n        if has_gpu:\n            device = torch.device(f'cuda:{ddp_context.local_rank}')\n            device_count = torch.cuda.device_count()\n            print(f'{i}/{n_iter}: GPU count: {device_count}')\n            device_name = torch.cuda.get_device_name(ddp_context.local_rank)\n            print(f'{i}/{n_iter}: GPU name: {device_name}')\n            device_property = torch.cuda.get_device_capability(device)\n            print(f'{i}/{n_iter}: GPU property: {device_property}')\n        else:\n            device = torch.device('cpu')\n        a_placed = a.to(device)\n        print(f'{i}/{n_iter}: Pre-`all_reduce` tensor: {a_placed}')\n        dist.all_reduce(a_placed)\n        print(f'{i}/{n_iter}: Post-`all_reduce` tensor: {a_placed}')\n        print('===================================================')\n    if duration is not None:\n        duration_seconds = n_iter * n_seconds_sleep\n        duration.assign(duration_seconds)\n\nfrom torch.distributed.elastic.multiprocessing.errors import record\n\ntensor_reduce=record(tensor_reduce)\n\nfrom bettmensch_ai.pipelines.component import as_torch_ddp\n\ntorch_ddp_decorator=as_torch_ddp()\n\ntorch_ddp_function=torch_ddp_decorator(tensor_reduce)\n\n\ntorch_ddp_function(n_iter,n_seconds_sleep,duration)", "name": "", "command": ["python"], "env": [{"name": "NCCL_DEBUG", "value": "INFO"}, {"name": "bettmensch_ai_torch_ddp_min_nodes", "value": "2"}, {"name": "bettmensch_ai_torch_ddp_max_nodes", "value": "2"}, {"name": "bettmensch_ai_torch_ddp_node_rank", "value": "1"}, {"name": "bettmensch_ai_torch_ddp_nproc_per_node", "value": "1"}, {"name": "bettmensch_ai_torch_ddp_max_restarts", "value": "1"}, {"name": "bettmensch_ai_torch_ddp_start_method", "value": "fork"}, {"name": "bettmensch_ai_torch_ddp_rdzv_backend", "value": "static"}, {"name": "bettmensch_ai_torch_ddp_rdzv_endpoint_url", "value": "torch-ddp-0-{{workflow.uid}}.argo.svc.cluster.local"}, {"name": "bettmensch_ai_torch_ddp_rdzv_endpoint_port", "value": "29200"}, {"name": "bettmensch_ai_torch_ddp_run_id", "value": "1"}, {"name": "bettmensch_ai_torch_ddp_tee", "value": "0"}], "resources": {"limits": {"cpu": "100m", "memory": "700Mi", "nvidia.com/gpu": "1"}, "requests": {"cpu": "100m", "memory": "700Mi", "nvidia.com/gpu": "1"}}, "image_pull_policy": "Always"}, "retry_strategy": {"limit": "1", "retry_policy": "OnError"}, "tolerations": [{"key": "nvidia.com/gpu", "operator": "Exists", "effect": "NoSchedule"}], "pod_spec_patch": "topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector:\n    matchExpressions:\n      - { key: torch-node, operator: In, values: ['0','1','2','3','4','5']}"}, {"name": "show-duration-param", "inputs": {"parameters": [{"name": "a"}]}, "outputs": {}, "metadata": {}, "script": {"image": "bettmensch88/bettmensch.ai-standard:3.11-latest", "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: a = json.loads(r'''{{inputs.parameters.a}}''')\nexcept: a = r'''{{inputs.parameters.a}}'''\n\nfrom bettmensch_ai.pipelines.io import InputParameter\n\ndef show_parameter(a: InputParameter) -> None:\n    \"\"\"When decorated with the bettmensch_ai.components.component decorator,\n    implements a bettmensch_ai.Component that prints the values of its\n    InputParameter.\"\"\"\n    print(f'Content of input parameter a is: {a}')\n\nshow_parameter(a)\n", "name": "", "command": ["python"], "resources": {"limits": {"cpu": "100m", "memory": "100Mi"}, "requests": {"cpu": "100m", "memory": "100Mi"}}, "image_pull_policy": "Always"}, "retry_strategy": {"limit": "1", "retry_policy": "OnError"}}, {"name": "bettmensch-ai-outer-dag", "inputs": {}, "outputs": {}, "metadata": {}, "dag": {"tasks": [{"name": "bettmensch-ai-inner-dag", "template": "bettmensch-ai-inner-dag", "arguments": {"parameters": [{"name": "n_iter", "value": "{{workflow.parameters.n_iter}}"}, {"name": "n_seconds_sleep", "value": "{{workflow.parameters.n_seconds_sleep}}"}]}}]}}], "entrypoint": "bettmensch-ai-outer-dag", "arguments": {"parameters": [{"name": "n_iter", "value": "15"}, {"name": "n_seconds_sleep", "value": "2"}]}, "service_account_name": "argo-workflow", "workflow_template_ref": {"name": "pipeline-test-torch-gpu-pipeline-7c4zp"}}, "artifact_repository_ref": {"config_map": "artifact-repositories", "key": "bettmensch-ai-artifact-repository", "namespace": "argo", "artifact_repository": {"s3": {"endpoint": "s3.us-east-2.amazonaws.com", "bucket": "bettmensch-ai-artifact-repository", "insecure": true, "key_format": "argo-workflows/{{workflow.name}}/{{pod.name}}"}}}, "artifact_gc_status": {"not_specified": true}, "task_results_completion_status": {"pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-1368447231": true, "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-1861925387": true, "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-2020597252": true, "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-41628430": true, "pipeline-test-torch-gpu-pipeline-7c4zp-flow-9ldcf-947069694": true}}}