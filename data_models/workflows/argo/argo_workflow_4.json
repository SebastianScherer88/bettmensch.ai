{
    "metadata": {
        "name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
        "generate_name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-",
        "namespace": "argo",
        "uid": "93098e5d-b8fe-4e2a-83d8-e19b7489c980",
        "resource_version": "13587",
        "generation": 13,
        "creation_timestamp": "test-datetime-value",
        "labels": {
            "workflows.argoproj.io/completed": "true",
            "workflows.argoproj.io/creator": "system-serviceaccount-argo-argo-server",
            "workflows.argoproj.io/phase": "Succeeded"
        },
        "annotations": {
            "karpenter.sh/do-not-disrupt": "true",
            "workflows.argoproj.io/pod-name-format": "v2"
        },
        "managed_fields": [
            {
                "manager": "argo",
                "operation": "Update",
                "api_version": "argoproj.io/v1alpha1",
                "time": "test-datetime-value",
                "fields_type": "FieldsV1",
                "fields_v1": {
                    "f:metadata": {
                        "f:generateName": {},
                        "f:labels": {
                            ".": {},
                            "f:workflows.argoproj.io/creator": {}
                        }
                    },
                    "f:spec": {}
                }
            },
            {
                "manager": "workflow-controller",
                "operation": "Update",
                "api_version": "argoproj.io/v1alpha1",
                "time": "test-datetime-value",
                "fields_type": "FieldsV1",
                "fields_v1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:karpenter.sh/do-not-disrupt": {},
                            "f:workflows.argoproj.io/pod-name-format": {}
                        },
                        "f:labels": {
                            "f:workflows.argoproj.io/completed": {},
                            "f:workflows.argoproj.io/phase": {}
                        }
                    },
                    "f:status": {}
                }
            }
        ]
    },
    "spec": {
        "arguments": {
            "parameters": [
                {
                    "name": "n_iter",
                    "value": "12"
                },
                {
                    "name": "n_seconds_sleep",
                    "value": "5"
                }
            ]
        },
        "workflow_template_ref": {
            "name": "pipeline-test-torch-gpu-pipeline-dcfq8"
        }
    },
    "status": {
        "phase": "Succeeded",
        "started_at": "test-datetime-value",
        "finished_at": "test-datetime-value",
        "progress": "7/7",
        "nodes": {
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx": {
                "id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
                "name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
                "type": "DAG",
                "display_name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
                "template_name": "bettmensch-ai-dag",
                "template_scope": "local/",
                "phase": "Succeeded",
                "started_at": "test-datetime-value",
                "finished_at": "test-datetime-value",
                "progress": "7/7",
                "resources_duration": {
                    "cpu": 57,
                    "memory": 4087,
                    "nvidia.com/gpu": 500
                },
                "children": [
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2966531784"
                ],
                "outbound_nodes": [
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-842282759",
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-1906221877",
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2953909358",
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2336401843",
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-1501533811"
                ]
            },
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-1501533811": {
                "id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-1501533811",
                "name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx.torch-ddp-delete-torch-service",
                "type": "Pod",
                "display_name": "torch-ddp-delete-torch-service",
                "template_name": "torch-ddp-delete-torch-service",
                "template_scope": "local/",
                "phase": "Succeeded",
                "boundary_id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
                "started_at": "test-datetime-value",
                "finished_at": "test-datetime-value",
                "progress": "1/1",
                "resources_duration": {
                    "cpu": 0,
                    "memory": 0
                },
                "outputs": {
                    "exit_code": "0"
                },
                "host_node_name": "ip-10-0-48-52.us-east-2.compute.internal"
            },
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-1664656268": {
                "id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-1664656268",
                "name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx.show-duration-param-0",
                "type": "Retry",
                "display_name": "show-duration-param-0",
                "template_name": "show-duration-param",
                "template_scope": "local/",
                "phase": "Succeeded",
                "boundary_id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
                "started_at": "test-datetime-value",
                "finished_at": "test-datetime-value",
                "progress": "1/1",
                "resources_duration": {
                    "cpu": 1,
                    "memory": 23
                },
                "inputs": {
                    "parameters": [
                        {
                            "name": "a",
                            "value": "60"
                        }
                    ]
                },
                "outputs": {
                    "exit_code": "0"
                },
                "children": [
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-842282759"
                ]
            },
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-1906221877": {
                "id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-1906221877",
                "name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx.torch-ddp-0-worker-1(0)",
                "type": "Pod",
                "display_name": "torch-ddp-0-worker-1(0)",
                "template_name": "torch-ddp-1",
                "template_scope": "local/",
                "phase": "Succeeded",
                "boundary_id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
                "started_at": "test-datetime-value",
                "finished_at": "test-datetime-value",
                "progress": "1/1",
                "resources_duration": {
                    "cpu": 14,
                    "memory": 1013,
                    "nvidia.com/gpu": 124
                },
                "node_flag": {
                    "retried": true
                },
                "inputs": {
                    "parameters": [
                        {
                            "name": "n_iter",
                            "default": "100",
                            "value": "12"
                        },
                        {
                            "name": "n_seconds_sleep",
                            "default": "10",
                            "value": "5"
                        },
                        {
                            "name": "duration",
                            "default": "null",
                            "value": "null"
                        }
                    ]
                },
                "outputs": {
                    "parameters": [
                        {
                            "name": "duration",
                            "value": "60",
                            "value_from": {
                                "path": "duration"
                            }
                        }
                    ],
                    "exit_code": "0"
                },
                "host_node_name": "ip-10-0-50-242.us-east-2.compute.internal"
            },
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-200409488": {
                "id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-200409488",
                "name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx.torch-ddp-0-worker-3",
                "type": "Retry",
                "display_name": "torch-ddp-0-worker-3",
                "template_name": "torch-ddp-3",
                "template_scope": "local/",
                "phase": "Succeeded",
                "boundary_id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
                "started_at": "test-datetime-value",
                "finished_at": "test-datetime-value",
                "progress": "1/1",
                "resources_duration": {
                    "cpu": 14,
                    "memory": 973,
                    "nvidia.com/gpu": 120
                },
                "inputs": {
                    "parameters": [
                        {
                            "name": "n_iter",
                            "default": "100",
                            "value": "12"
                        },
                        {
                            "name": "n_seconds_sleep",
                            "default": "10",
                            "value": "5"
                        },
                        {
                            "name": "duration",
                            "default": "null",
                            "value": "null"
                        }
                    ]
                },
                "outputs": {
                    "parameters": [
                        {
                            "name": "duration",
                            "value": "60",
                            "value_from": {
                                "path": "duration"
                            }
                        }
                    ],
                    "exit_code": "0"
                },
                "children": [
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2336401843"
                ]
            },
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-217187107": {
                "id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-217187107",
                "name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx.torch-ddp-0-worker-2",
                "type": "Retry",
                "display_name": "torch-ddp-0-worker-2",
                "template_name": "torch-ddp-2",
                "template_scope": "local/",
                "phase": "Succeeded",
                "boundary_id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
                "started_at": "test-datetime-value",
                "finished_at": "test-datetime-value",
                "progress": "1/1",
                "resources_duration": {
                    "cpu": 13,
                    "memory": 966,
                    "nvidia.com/gpu": 118
                },
                "inputs": {
                    "parameters": [
                        {
                            "name": "n_iter",
                            "default": "100",
                            "value": "12"
                        },
                        {
                            "name": "n_seconds_sleep",
                            "default": "10",
                            "value": "5"
                        },
                        {
                            "name": "duration",
                            "default": "null",
                            "value": "null"
                        }
                    ]
                },
                "outputs": {
                    "parameters": [
                        {
                            "name": "duration",
                            "value": "60",
                            "value_from": {
                                "path": "duration"
                            }
                        }
                    ],
                    "exit_code": "0"
                },
                "children": [
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2953909358"
                ]
            },
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2258088662": {
                "id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2258088662",
                "name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx.torch-ddp-0(0)",
                "type": "Pod",
                "display_name": "torch-ddp-0(0)",
                "template_name": "torch-ddp-0",
                "template_scope": "local/",
                "phase": "Succeeded",
                "boundary_id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
                "started_at": "test-datetime-value",
                "finished_at": "test-datetime-value",
                "progress": "1/1",
                "resources_duration": {
                    "cpu": 15,
                    "memory": 1112,
                    "nvidia.com/gpu": 138
                },
                "node_flag": {
                    "retried": true
                },
                "inputs": {
                    "parameters": [
                        {
                            "name": "n_iter",
                            "default": "100",
                            "value": "12"
                        },
                        {
                            "name": "n_seconds_sleep",
                            "default": "10",
                            "value": "5"
                        },
                        {
                            "name": "duration",
                            "default": "null",
                            "value": "null"
                        }
                    ]
                },
                "outputs": {
                    "parameters": [
                        {
                            "name": "duration",
                            "value": "60",
                            "value_from": {
                                "path": "duration"
                            }
                        }
                    ],
                    "exit_code": "0"
                },
                "children": [
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-1664656268",
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-1501533811"
                ],
                "host_node_name": "ip-10-0-49-47.us-east-2.compute.internal"
            },
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2336401843": {
                "id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2336401843",
                "name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx.torch-ddp-0-worker-3(0)",
                "type": "Pod",
                "display_name": "torch-ddp-0-worker-3(0)",
                "template_name": "torch-ddp-3",
                "template_scope": "local/",
                "phase": "Succeeded",
                "boundary_id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
                "started_at": "test-datetime-value",
                "finished_at": "test-datetime-value",
                "progress": "1/1",
                "resources_duration": {
                    "cpu": 14,
                    "memory": 973,
                    "nvidia.com/gpu": 120
                },
                "node_flag": {
                    "retried": true
                },
                "inputs": {
                    "parameters": [
                        {
                            "name": "n_iter",
                            "default": "100",
                            "value": "12"
                        },
                        {
                            "name": "n_seconds_sleep",
                            "default": "10",
                            "value": "5"
                        },
                        {
                            "name": "duration",
                            "default": "null",
                            "value": "null"
                        }
                    ]
                },
                "outputs": {
                    "parameters": [
                        {
                            "name": "duration",
                            "value": "60",
                            "value_from": {
                                "path": "duration"
                            }
                        }
                    ],
                    "exit_code": "0"
                },
                "host_node_name": "ip-10-0-49-43.us-east-2.compute.internal"
            },
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-233964726": {
                "id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-233964726",
                "name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx.torch-ddp-0-worker-1",
                "type": "Retry",
                "display_name": "torch-ddp-0-worker-1",
                "template_name": "torch-ddp-1",
                "template_scope": "local/",
                "phase": "Succeeded",
                "boundary_id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
                "started_at": "test-datetime-value",
                "finished_at": "test-datetime-value",
                "progress": "1/1",
                "resources_duration": {
                    "cpu": 14,
                    "memory": 1013,
                    "nvidia.com/gpu": 124
                },
                "inputs": {
                    "parameters": [
                        {
                            "name": "n_iter",
                            "default": "100",
                            "value": "12"
                        },
                        {
                            "name": "n_seconds_sleep",
                            "default": "10",
                            "value": "5"
                        },
                        {
                            "name": "duration",
                            "default": "null",
                            "value": "null"
                        }
                    ]
                },
                "outputs": {
                    "parameters": [
                        {
                            "name": "duration",
                            "value": "60",
                            "value_from": {
                                "path": "duration"
                            }
                        }
                    ],
                    "exit_code": "0"
                },
                "children": [
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-1906221877"
                ]
            },
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2953909358": {
                "id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2953909358",
                "name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx.torch-ddp-0-worker-2(0)",
                "type": "Pod",
                "display_name": "torch-ddp-0-worker-2(0)",
                "template_name": "torch-ddp-2",
                "template_scope": "local/",
                "phase": "Succeeded",
                "boundary_id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
                "started_at": "test-datetime-value",
                "finished_at": "test-datetime-value",
                "progress": "1/1",
                "resources_duration": {
                    "cpu": 13,
                    "memory": 966,
                    "nvidia.com/gpu": 118
                },
                "node_flag": {
                    "retried": true
                },
                "inputs": {
                    "parameters": [
                        {
                            "name": "n_iter",
                            "default": "100",
                            "value": "12"
                        },
                        {
                            "name": "n_seconds_sleep",
                            "default": "10",
                            "value": "5"
                        },
                        {
                            "name": "duration",
                            "default": "null",
                            "value": "null"
                        }
                    ]
                },
                "outputs": {
                    "parameters": [
                        {
                            "name": "duration",
                            "value": "60",
                            "value_from": {
                                "path": "duration"
                            }
                        }
                    ],
                    "exit_code": "0"
                },
                "host_node_name": "ip-10-0-50-184.us-east-2.compute.internal"
            },
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2966531784": {
                "id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2966531784",
                "name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx.torch-ddp-create-torch-service",
                "type": "Pod",
                "display_name": "torch-ddp-create-torch-service",
                "template_name": "torch-ddp-create-torch-service",
                "template_scope": "local/",
                "phase": "Succeeded",
                "boundary_id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
                "started_at": "test-datetime-value",
                "finished_at": "test-datetime-value",
                "progress": "1/1",
                "resources_duration": {
                    "cpu": 0,
                    "memory": 0
                },
                "outputs": {
                    "exit_code": "0"
                },
                "children": [
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-3686612827",
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-233964726",
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-217187107",
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-200409488"
                ],
                "host_node_name": "ip-10-0-48-52.us-east-2.compute.internal"
            },
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-3686612827": {
                "id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-3686612827",
                "name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx.torch-ddp-0",
                "type": "Retry",
                "display_name": "torch-ddp-0",
                "template_name": "torch-ddp-0",
                "template_scope": "local/",
                "phase": "Succeeded",
                "boundary_id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
                "started_at": "test-datetime-value",
                "finished_at": "test-datetime-value",
                "progress": "3/3",
                "resources_duration": {
                    "cpu": 16,
                    "memory": 1135,
                    "nvidia.com/gpu": 138
                },
                "inputs": {
                    "parameters": [
                        {
                            "name": "n_iter",
                            "default": "100",
                            "value": "12"
                        },
                        {
                            "name": "n_seconds_sleep",
                            "default": "10",
                            "value": "5"
                        },
                        {
                            "name": "duration",
                            "default": "null",
                            "value": "null"
                        }
                    ]
                },
                "outputs": {
                    "parameters": [
                        {
                            "name": "duration",
                            "value": "60",
                            "value_from": {
                                "path": "duration"
                            }
                        }
                    ],
                    "exit_code": "0"
                },
                "children": [
                    "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2258088662"
                ]
            },
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-842282759": {
                "id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-842282759",
                "name": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx.show-duration-param-0(0)",
                "type": "Pod",
                "display_name": "show-duration-param-0(0)",
                "template_name": "show-duration-param",
                "template_scope": "local/",
                "phase": "Succeeded",
                "boundary_id": "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx",
                "started_at": "test-datetime-value",
                "finished_at": "test-datetime-value",
                "progress": "1/1",
                "resources_duration": {
                    "cpu": 1,
                    "memory": 23
                },
                "node_flag": {
                    "retried": true
                },
                "inputs": {
                    "parameters": [
                        {
                            "name": "a",
                            "value": "60"
                        }
                    ]
                },
                "outputs": {
                    "exit_code": "0"
                },
                "host_node_name": "ip-10-0-48-52.us-east-2.compute.internal"
            }
        },
        "stored_templates": {
            "namespaced/pipeline-test-torch-gpu-pipeline-dcfq8/bettmensch-ai-dag": {
                "name": "bettmensch-ai-dag",
                "inputs": {},
                "outputs": {},
                "metadata": {},
                "dag": {
                    "tasks": [
                        {
                            "name": "torch-ddp-create-torch-service",
                            "template": "torch-ddp-create-torch-service",
                            "arguments": {}
                        },
                        {
                            "name": "torch-ddp-0",
                            "template": "torch-ddp-0",
                            "arguments": {
                                "parameters": [
                                    {
                                        "name": "n_iter",
                                        "value": "{{workflow.parameters.n_iter}}"
                                    },
                                    {
                                        "name": "n_seconds_sleep",
                                        "value": "{{workflow.parameters.n_seconds_sleep}}"
                                    }
                                ]
                            },
                            "depends": "torch-ddp-create-torch-service"
                        },
                        {
                            "name": "torch-ddp-0-worker-1",
                            "template": "torch-ddp-1",
                            "arguments": {
                                "parameters": [
                                    {
                                        "name": "n_iter",
                                        "value": "{{workflow.parameters.n_iter}}"
                                    },
                                    {
                                        "name": "n_seconds_sleep",
                                        "value": "{{workflow.parameters.n_seconds_sleep}}"
                                    }
                                ]
                            },
                            "depends": "torch-ddp-create-torch-service"
                        },
                        {
                            "name": "torch-ddp-0-worker-2",
                            "template": "torch-ddp-2",
                            "arguments": {
                                "parameters": [
                                    {
                                        "name": "n_iter",
                                        "value": "{{workflow.parameters.n_iter}}"
                                    },
                                    {
                                        "name": "n_seconds_sleep",
                                        "value": "{{workflow.parameters.n_seconds_sleep}}"
                                    }
                                ]
                            },
                            "depends": "torch-ddp-create-torch-service"
                        },
                        {
                            "name": "torch-ddp-0-worker-3",
                            "template": "torch-ddp-3",
                            "arguments": {
                                "parameters": [
                                    {
                                        "name": "n_iter",
                                        "value": "{{workflow.parameters.n_iter}}"
                                    },
                                    {
                                        "name": "n_seconds_sleep",
                                        "value": "{{workflow.parameters.n_seconds_sleep}}"
                                    }
                                ]
                            },
                            "depends": "torch-ddp-create-torch-service"
                        },
                        {
                            "name": "torch-ddp-delete-torch-service",
                            "template": "torch-ddp-delete-torch-service",
                            "arguments": {},
                            "depends": "torch-ddp-0"
                        },
                        {
                            "name": "show-duration-param-0",
                            "template": "show-duration-param",
                            "arguments": {
                                "parameters": [
                                    {
                                        "name": "a",
                                        "value": "{{tasks.torch-ddp-0.outputs.parameters.duration}}"
                                    }
                                ]
                            },
                            "depends": "torch-ddp-0"
                        }
                    ]
                }
            },
            "namespaced/pipeline-test-torch-gpu-pipeline-dcfq8/show-duration-param": {
                "name": "show-duration-param",
                "inputs": {
                    "parameters": [
                        {
                            "name": "a"
                        }
                    ]
                },
                "outputs": {},
                "metadata": {},
                "script": {
                    "image": "bettmensch88/bettmensch.ai:3.11-latest",
                    "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: a = json.loads(r'''{{inputs.parameters.a}}''')\nexcept: a = r'''{{inputs.parameters.a}}'''\n\nfrom bettmensch_ai.io import InputParameter\n\ndef show_parameter(a: InputParameter) -> None:\n    \"\"\"When decorated with the bettmensch_ai.components.component decorator,\n    implements a bettmensch_ai.Component that prints the values of its\n    InputParameter.\"\"\"\n    print(f'Content of input parameter a is: {a}')\nshow_parameter(a)",
                    "name": "",
                    "command": [
                        "python"
                    ],
                    "resources": {
                        "limits": {
                            "cpu": "100m",
                            "memory": "100Mi"
                        },
                        "requests": {
                            "cpu": "100m",
                            "memory": "100Mi"
                        }
                    },
                    "image_pull_policy": "Always"
                },
                "retry_strategy": {
                    "limit": "1",
                    "retry_policy": "OnError"
                }
            },
            "namespaced/pipeline-test-torch-gpu-pipeline-dcfq8/torch-ddp-0": {
                "name": "torch-ddp-0",
                "inputs": {
                    "parameters": [
                        {
                            "name": "n_iter",
                            "default": "100"
                        },
                        {
                            "name": "n_seconds_sleep",
                            "default": "10"
                        },
                        {
                            "name": "duration",
                            "default": "null"
                        }
                    ]
                },
                "outputs": {
                    "parameters": [
                        {
                            "name": "duration",
                            "value_from": {
                                "path": "duration"
                            }
                        }
                    ]
                },
                "metadata": {
                    "labels": {
                        "torch-job": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6",
                        "torch-node": "0"
                    }
                },
                "script": {
                    "image": "bettmensch88/bettmensch.ai-torch:3.11-latest",
                    "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: n_iter = json.loads(r'''{{inputs.parameters.n_iter}}''')\nexcept: n_iter = r'''{{inputs.parameters.n_iter}}'''\ntry: n_seconds_sleep = json.loads(r'''{{inputs.parameters.n_seconds_sleep}}''')\nexcept: n_seconds_sleep = r'''{{inputs.parameters.n_seconds_sleep}}'''\n\nfrom bettmensch_ai.io import InputParameter\n\nfrom bettmensch_ai.io import OutputParameter\nduration = OutputParameter(\"duration\")\n\ndef torch_ddp(n_iter: InputParameter=100, n_seconds_sleep: InputParameter=10, duration: OutputParameter=None) -> None:\n    \"\"\"When decorated with the torch_component decorator, implements a\n    bettmensch_ai.TorchComponent that runs a torch DDP across pods and nodes in\n    your K8s cluster.\"\"\"\n    import time\n    from datetime import datetime as dt\n    import torch\n    import torch.distributed as dist\n    has_gpu = torch.cuda.is_available()\n    print(f'GPU present: {has_gpu}')\n    if has_gpu:\n        dist.init_process_group(backend='nccl')\n    else:\n        dist.init_process_group(backend='gloo')\n    for i in range(1, n_iter + 1):\n        time.sleep(n_seconds_sleep)\n        a = torch.tensor([dist.get_rank()])\n        print(f'{i}/{n_iter}: @{dt.now()}')\n        print(f'{i}/{n_iter}: Backend {dist.get_backend()}')\n        print(f'{i}/{n_iter}: World size {dist.get_world_size()}')\n        print(f'{i}/{n_iter}: Rank {dist.get_rank()}')\n        print(f'{i}/{n_iter}: This makes me worker process {dist.get_rank() + 1}/{dist.get_world_size()} globally!')\n        if has_gpu:\n            device = torch.device('cuda:0')\n            device_count = torch.cuda.device_count()\n            print(f'{i}/{n_iter}: GPU count: {device_count}')\n            device_name = torch.cuda.get_device_name(0)\n            print(f'{i}/{n_iter}: GPU name: {device_name}')\n            device_property = torch.cuda.get_device_capability(device)\n            print(f'{i}/{n_iter}: GPU property: {device_property}')\n        else:\n            device = torch.device('cpu')\n        a_placed = a.to(device)\n        print(f'{i}/{n_iter}: Pre-`all_reduce` tensor: {a_placed}')\n        dist.all_reduce(a_placed)\n        print(f'{i}/{n_iter}: Post-`all_reduce` tensor: {a_placed}')\n        print('===================================================')\n    if duration is not None:\n        duration_seconds = n_iter * n_seconds_sleep\n        duration.assign(duration_seconds)\n\nfrom bettmensch_ai.components import torch_distribute\n\ntorch_distribute_decorator=torch_distribute()\ntorch_distributed_function=torch_distribute_decorator(torch_ddp)\n\ntorch_distributed_function(n_iter,n_seconds_sleep,duration)",
                    "name": "",
                    "command": [
                        "python"
                    ],
                    "ports": [
                        {
                            "container_port": 29200,
                            "name": "ddp",
                            "protocol": "TCP"
                        }
                    ],
                    "env": [
                        {
                            "name": "NCCL_DEBUG",
                            "value": "INFO"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_min_nodes",
                            "value": "4"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_max_nodes",
                            "value": "4"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_node_rank",
                            "value": "0"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_nproc_per_node",
                            "value": "1"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_max_restarts",
                            "value": "1"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_start_method",
                            "value": "fork"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_rdzv_backend",
                            "value": "static"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_url",
                            "value": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6.argo.svc.cluster.local"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_port",
                            "value": "29200"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_run_id",
                            "value": "1"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_tee",
                            "value": "0"
                        }
                    ],
                    "resources": {
                        "limits": {
                            "cpu": "100m",
                            "memory": "700Mi",
                            "nvidia.com/gpu": "1"
                        },
                        "requests": {
                            "cpu": "100m",
                            "memory": "700Mi",
                            "nvidia.com/gpu": "1"
                        }
                    },
                    "image_pull_policy": "Always"
                },
                "retry_strategy": {
                    "limit": "1",
                    "retry_policy": "OnError"
                },
                "tolerations": [
                    {
                        "key": "nvidia.com/gpu",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    }
                ],
                "pod_spec_patch": "topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector:\n    matchExpressions:\n      - { key: torch-node, operator: In, values: ['0','1','2','3','4','5']}"
            },
            "namespaced/pipeline-test-torch-gpu-pipeline-dcfq8/torch-ddp-1": {
                "name": "torch-ddp-1",
                "inputs": {
                    "parameters": [
                        {
                            "name": "n_iter",
                            "default": "100"
                        },
                        {
                            "name": "n_seconds_sleep",
                            "default": "10"
                        },
                        {
                            "name": "duration",
                            "default": "null"
                        }
                    ]
                },
                "outputs": {
                    "parameters": [
                        {
                            "name": "duration",
                            "value_from": {
                                "path": "duration"
                            }
                        }
                    ]
                },
                "metadata": {
                    "labels": {
                        "torch-job": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6",
                        "torch-node": "1"
                    }
                },
                "script": {
                    "image": "bettmensch88/bettmensch.ai-torch:3.11-latest",
                    "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: n_iter = json.loads(r'''{{inputs.parameters.n_iter}}''')\nexcept: n_iter = r'''{{inputs.parameters.n_iter}}'''\ntry: n_seconds_sleep = json.loads(r'''{{inputs.parameters.n_seconds_sleep}}''')\nexcept: n_seconds_sleep = r'''{{inputs.parameters.n_seconds_sleep}}'''\n\nfrom bettmensch_ai.io import InputParameter\n\nfrom bettmensch_ai.io import OutputParameter\nduration = OutputParameter(\"duration\")\n\ndef torch_ddp(n_iter: InputParameter=100, n_seconds_sleep: InputParameter=10, duration: OutputParameter=None) -> None:\n    \"\"\"When decorated with the torch_component decorator, implements a\n    bettmensch_ai.TorchComponent that runs a torch DDP across pods and nodes in\n    your K8s cluster.\"\"\"\n    import time\n    from datetime import datetime as dt\n    import torch\n    import torch.distributed as dist\n    has_gpu = torch.cuda.is_available()\n    print(f'GPU present: {has_gpu}')\n    if has_gpu:\n        dist.init_process_group(backend='nccl')\n    else:\n        dist.init_process_group(backend='gloo')\n    for i in range(1, n_iter + 1):\n        time.sleep(n_seconds_sleep)\n        a = torch.tensor([dist.get_rank()])\n        print(f'{i}/{n_iter}: @{dt.now()}')\n        print(f'{i}/{n_iter}: Backend {dist.get_backend()}')\n        print(f'{i}/{n_iter}: World size {dist.get_world_size()}')\n        print(f'{i}/{n_iter}: Rank {dist.get_rank()}')\n        print(f'{i}/{n_iter}: This makes me worker process {dist.get_rank() + 1}/{dist.get_world_size()} globally!')\n        if has_gpu:\n            device = torch.device('cuda:0')\n            device_count = torch.cuda.device_count()\n            print(f'{i}/{n_iter}: GPU count: {device_count}')\n            device_name = torch.cuda.get_device_name(0)\n            print(f'{i}/{n_iter}: GPU name: {device_name}')\n            device_property = torch.cuda.get_device_capability(device)\n            print(f'{i}/{n_iter}: GPU property: {device_property}')\n        else:\n            device = torch.device('cpu')\n        a_placed = a.to(device)\n        print(f'{i}/{n_iter}: Pre-`all_reduce` tensor: {a_placed}')\n        dist.all_reduce(a_placed)\n        print(f'{i}/{n_iter}: Post-`all_reduce` tensor: {a_placed}')\n        print('===================================================')\n    if duration is not None:\n        duration_seconds = n_iter * n_seconds_sleep\n        duration.assign(duration_seconds)\n\nfrom bettmensch_ai.components import torch_distribute\n\ntorch_distribute_decorator=torch_distribute()\ntorch_distributed_function=torch_distribute_decorator(torch_ddp)\n\ntorch_distributed_function(n_iter,n_seconds_sleep,duration)",
                    "name": "",
                    "command": [
                        "python"
                    ],
                    "env": [
                        {
                            "name": "NCCL_DEBUG",
                            "value": "INFO"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_min_nodes",
                            "value": "4"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_max_nodes",
                            "value": "4"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_node_rank",
                            "value": "1"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_nproc_per_node",
                            "value": "1"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_max_restarts",
                            "value": "1"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_start_method",
                            "value": "fork"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_rdzv_backend",
                            "value": "static"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_url",
                            "value": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6.argo.svc.cluster.local"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_port",
                            "value": "29200"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_run_id",
                            "value": "1"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_tee",
                            "value": "0"
                        }
                    ],
                    "resources": {
                        "limits": {
                            "cpu": "100m",
                            "memory": "700Mi",
                            "nvidia.com/gpu": "1"
                        },
                        "requests": {
                            "cpu": "100m",
                            "memory": "700Mi",
                            "nvidia.com/gpu": "1"
                        }
                    },
                    "image_pull_policy": "Always"
                },
                "retry_strategy": {
                    "limit": "1",
                    "retry_policy": "OnError"
                },
                "tolerations": [
                    {
                        "key": "nvidia.com/gpu",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    }
                ],
                "pod_spec_patch": "topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector:\n    matchExpressions:\n      - { key: torch-node, operator: In, values: ['0','1','2','3','4','5']}"
            },
            "namespaced/pipeline-test-torch-gpu-pipeline-dcfq8/torch-ddp-2": {
                "name": "torch-ddp-2",
                "inputs": {
                    "parameters": [
                        {
                            "name": "n_iter",
                            "default": "100"
                        },
                        {
                            "name": "n_seconds_sleep",
                            "default": "10"
                        },
                        {
                            "name": "duration",
                            "default": "null"
                        }
                    ]
                },
                "outputs": {
                    "parameters": [
                        {
                            "name": "duration",
                            "value_from": {
                                "path": "duration"
                            }
                        }
                    ]
                },
                "metadata": {
                    "labels": {
                        "torch-job": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6",
                        "torch-node": "2"
                    }
                },
                "script": {
                    "image": "bettmensch88/bettmensch.ai-torch:3.11-latest",
                    "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: n_iter = json.loads(r'''{{inputs.parameters.n_iter}}''')\nexcept: n_iter = r'''{{inputs.parameters.n_iter}}'''\ntry: n_seconds_sleep = json.loads(r'''{{inputs.parameters.n_seconds_sleep}}''')\nexcept: n_seconds_sleep = r'''{{inputs.parameters.n_seconds_sleep}}'''\n\nfrom bettmensch_ai.io import InputParameter\n\nfrom bettmensch_ai.io import OutputParameter\nduration = OutputParameter(\"duration\")\n\ndef torch_ddp(n_iter: InputParameter=100, n_seconds_sleep: InputParameter=10, duration: OutputParameter=None) -> None:\n    \"\"\"When decorated with the torch_component decorator, implements a\n    bettmensch_ai.TorchComponent that runs a torch DDP across pods and nodes in\n    your K8s cluster.\"\"\"\n    import time\n    from datetime import datetime as dt\n    import torch\n    import torch.distributed as dist\n    has_gpu = torch.cuda.is_available()\n    print(f'GPU present: {has_gpu}')\n    if has_gpu:\n        dist.init_process_group(backend='nccl')\n    else:\n        dist.init_process_group(backend='gloo')\n    for i in range(1, n_iter + 1):\n        time.sleep(n_seconds_sleep)\n        a = torch.tensor([dist.get_rank()])\n        print(f'{i}/{n_iter}: @{dt.now()}')\n        print(f'{i}/{n_iter}: Backend {dist.get_backend()}')\n        print(f'{i}/{n_iter}: World size {dist.get_world_size()}')\n        print(f'{i}/{n_iter}: Rank {dist.get_rank()}')\n        print(f'{i}/{n_iter}: This makes me worker process {dist.get_rank() + 1}/{dist.get_world_size()} globally!')\n        if has_gpu:\n            device = torch.device('cuda:0')\n            device_count = torch.cuda.device_count()\n            print(f'{i}/{n_iter}: GPU count: {device_count}')\n            device_name = torch.cuda.get_device_name(0)\n            print(f'{i}/{n_iter}: GPU name: {device_name}')\n            device_property = torch.cuda.get_device_capability(device)\n            print(f'{i}/{n_iter}: GPU property: {device_property}')\n        else:\n            device = torch.device('cpu')\n        a_placed = a.to(device)\n        print(f'{i}/{n_iter}: Pre-`all_reduce` tensor: {a_placed}')\n        dist.all_reduce(a_placed)\n        print(f'{i}/{n_iter}: Post-`all_reduce` tensor: {a_placed}')\n        print('===================================================')\n    if duration is not None:\n        duration_seconds = n_iter * n_seconds_sleep\n        duration.assign(duration_seconds)\n\nfrom bettmensch_ai.components import torch_distribute\n\ntorch_distribute_decorator=torch_distribute()\ntorch_distributed_function=torch_distribute_decorator(torch_ddp)\n\ntorch_distributed_function(n_iter,n_seconds_sleep,duration)",
                    "name": "",
                    "command": [
                        "python"
                    ],
                    "env": [
                        {
                            "name": "NCCL_DEBUG",
                            "value": "INFO"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_min_nodes",
                            "value": "4"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_max_nodes",
                            "value": "4"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_node_rank",
                            "value": "2"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_nproc_per_node",
                            "value": "1"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_max_restarts",
                            "value": "1"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_start_method",
                            "value": "fork"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_rdzv_backend",
                            "value": "static"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_url",
                            "value": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6.argo.svc.cluster.local"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_port",
                            "value": "29200"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_run_id",
                            "value": "1"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_tee",
                            "value": "0"
                        }
                    ],
                    "resources": {
                        "limits": {
                            "cpu": "100m",
                            "memory": "700Mi",
                            "nvidia.com/gpu": "1"
                        },
                        "requests": {
                            "cpu": "100m",
                            "memory": "700Mi",
                            "nvidia.com/gpu": "1"
                        }
                    },
                    "image_pull_policy": "Always"
                },
                "retry_strategy": {
                    "limit": "1",
                    "retry_policy": "OnError"
                },
                "tolerations": [
                    {
                        "key": "nvidia.com/gpu",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    }
                ],
                "pod_spec_patch": "topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector:\n    matchExpressions:\n      - { key: torch-node, operator: In, values: ['0','1','2','3','4','5']}"
            },
            "namespaced/pipeline-test-torch-gpu-pipeline-dcfq8/torch-ddp-3": {
                "name": "torch-ddp-3",
                "inputs": {
                    "parameters": [
                        {
                            "name": "n_iter",
                            "default": "100"
                        },
                        {
                            "name": "n_seconds_sleep",
                            "default": "10"
                        },
                        {
                            "name": "duration",
                            "default": "null"
                        }
                    ]
                },
                "outputs": {
                    "parameters": [
                        {
                            "name": "duration",
                            "value_from": {
                                "path": "duration"
                            }
                        }
                    ]
                },
                "metadata": {
                    "labels": {
                        "torch-job": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6",
                        "torch-node": "3"
                    }
                },
                "script": {
                    "image": "bettmensch88/bettmensch.ai-torch:3.11-latest",
                    "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: n_iter = json.loads(r'''{{inputs.parameters.n_iter}}''')\nexcept: n_iter = r'''{{inputs.parameters.n_iter}}'''\ntry: n_seconds_sleep = json.loads(r'''{{inputs.parameters.n_seconds_sleep}}''')\nexcept: n_seconds_sleep = r'''{{inputs.parameters.n_seconds_sleep}}'''\n\nfrom bettmensch_ai.io import InputParameter\n\nfrom bettmensch_ai.io import OutputParameter\nduration = OutputParameter(\"duration\")\n\ndef torch_ddp(n_iter: InputParameter=100, n_seconds_sleep: InputParameter=10, duration: OutputParameter=None) -> None:\n    \"\"\"When decorated with the torch_component decorator, implements a\n    bettmensch_ai.TorchComponent that runs a torch DDP across pods and nodes in\n    your K8s cluster.\"\"\"\n    import time\n    from datetime import datetime as dt\n    import torch\n    import torch.distributed as dist\n    has_gpu = torch.cuda.is_available()\n    print(f'GPU present: {has_gpu}')\n    if has_gpu:\n        dist.init_process_group(backend='nccl')\n    else:\n        dist.init_process_group(backend='gloo')\n    for i in range(1, n_iter + 1):\n        time.sleep(n_seconds_sleep)\n        a = torch.tensor([dist.get_rank()])\n        print(f'{i}/{n_iter}: @{dt.now()}')\n        print(f'{i}/{n_iter}: Backend {dist.get_backend()}')\n        print(f'{i}/{n_iter}: World size {dist.get_world_size()}')\n        print(f'{i}/{n_iter}: Rank {dist.get_rank()}')\n        print(f'{i}/{n_iter}: This makes me worker process {dist.get_rank() + 1}/{dist.get_world_size()} globally!')\n        if has_gpu:\n            device = torch.device('cuda:0')\n            device_count = torch.cuda.device_count()\n            print(f'{i}/{n_iter}: GPU count: {device_count}')\n            device_name = torch.cuda.get_device_name(0)\n            print(f'{i}/{n_iter}: GPU name: {device_name}')\n            device_property = torch.cuda.get_device_capability(device)\n            print(f'{i}/{n_iter}: GPU property: {device_property}')\n        else:\n            device = torch.device('cpu')\n        a_placed = a.to(device)\n        print(f'{i}/{n_iter}: Pre-`all_reduce` tensor: {a_placed}')\n        dist.all_reduce(a_placed)\n        print(f'{i}/{n_iter}: Post-`all_reduce` tensor: {a_placed}')\n        print('===================================================')\n    if duration is not None:\n        duration_seconds = n_iter * n_seconds_sleep\n        duration.assign(duration_seconds)\n\nfrom bettmensch_ai.components import torch_distribute\n\ntorch_distribute_decorator=torch_distribute()\ntorch_distributed_function=torch_distribute_decorator(torch_ddp)\n\ntorch_distributed_function(n_iter,n_seconds_sleep,duration)",
                    "name": "",
                    "command": [
                        "python"
                    ],
                    "env": [
                        {
                            "name": "NCCL_DEBUG",
                            "value": "INFO"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_min_nodes",
                            "value": "4"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_max_nodes",
                            "value": "4"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_node_rank",
                            "value": "3"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_nproc_per_node",
                            "value": "1"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_max_restarts",
                            "value": "1"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_start_method",
                            "value": "fork"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_rdzv_backend",
                            "value": "static"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_url",
                            "value": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6.argo.svc.cluster.local"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_port",
                            "value": "29200"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_run_id",
                            "value": "1"
                        },
                        {
                            "name": "bettmensch_ai_distributed_torch_tee",
                            "value": "0"
                        }
                    ],
                    "resources": {
                        "limits": {
                            "cpu": "100m",
                            "memory": "700Mi",
                            "nvidia.com/gpu": "1"
                        },
                        "requests": {
                            "cpu": "100m",
                            "memory": "700Mi",
                            "nvidia.com/gpu": "1"
                        }
                    },
                    "image_pull_policy": "Always"
                },
                "retry_strategy": {
                    "limit": "1",
                    "retry_policy": "OnError"
                },
                "tolerations": [
                    {
                        "key": "nvidia.com/gpu",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    }
                ],
                "pod_spec_patch": "topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector:\n    matchExpressions:\n      - { key: torch-node, operator: In, values: ['0','1','2','3','4','5']}"
            },
            "namespaced/pipeline-test-torch-gpu-pipeline-dcfq8/torch-ddp-create-torch-service": {
                "name": "torch-ddp-create-torch-service",
                "inputs": {},
                "outputs": {},
                "metadata": {},
                "resource": {
                    "action": "create",
                    "manifest": "apiVersion: v1\nkind: Service\nmetadata:\n  name: torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6\n  namespace: argo\n  labels:\n    app: torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6\nspec:\n  clusterIP: None  # ClusterIP set to None for headless service.\n  ports:\n  - name: ddp  # Port for torchrun master<->worker node coms.\n    port: 29200\n    targetPort: 29200\n  selector:\n    torch-job: torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6\n    torch-node: '0'  # Selector for pods associated with this service.\n"
                }
            },
            "namespaced/pipeline-test-torch-gpu-pipeline-dcfq8/torch-ddp-delete-torch-service": {
                "name": "torch-ddp-delete-torch-service",
                "inputs": {},
                "outputs": {},
                "metadata": {},
                "resource": {
                    "action": "delete",
                    "flags": [
                        "service",
                        "--selector",
                        "torch-job=torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6",
                        "-n",
                        "argo"
                    ]
                }
            }
        },
        "conditions": [
            {
                "type": "PodRunning",
                "status": "False"
            },
            {
                "type": "Completed",
                "status": "True"
            }
        ],
        "resources_duration": {
            "cpu": 57,
            "memory": 4087,
            "nvidia.com/gpu": 500
        },
        "stored_workflow_template_spec": {
            "templates": [
                {
                    "name": "torch-ddp-create-torch-service",
                    "inputs": {},
                    "outputs": {},
                    "metadata": {},
                    "resource": {
                        "action": "create",
                        "manifest": "apiVersion: v1\nkind: Service\nmetadata:\n  name: torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6\n  namespace: argo\n  labels:\n    app: torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6\nspec:\n  clusterIP: None  # ClusterIP set to None for headless service.\n  ports:\n  - name: ddp  # Port for torchrun master<->worker node coms.\n    port: 29200\n    targetPort: 29200\n  selector:\n    torch-job: torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6\n    torch-node: '0'  # Selector for pods associated with this service.\n"
                    }
                },
                {
                    "name": "torch-ddp-delete-torch-service",
                    "inputs": {},
                    "outputs": {},
                    "metadata": {},
                    "resource": {
                        "action": "delete",
                        "flags": [
                            "service",
                            "--selector",
                            "torch-job=torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6",
                            "-n",
                            "argo"
                        ]
                    }
                },
                {
                    "name": "bettmensch-ai-dag",
                    "inputs": {},
                    "outputs": {},
                    "metadata": {},
                    "dag": {
                        "tasks": [
                            {
                                "name": "torch-ddp-create-torch-service",
                                "template": "torch-ddp-create-torch-service",
                                "arguments": {}
                            },
                            {
                                "name": "torch-ddp-0",
                                "template": "torch-ddp-0",
                                "arguments": {
                                    "parameters": [
                                        {
                                            "name": "n_iter",
                                            "value": "{{workflow.parameters.n_iter}}"
                                        },
                                        {
                                            "name": "n_seconds_sleep",
                                            "value": "{{workflow.parameters.n_seconds_sleep}}"
                                        }
                                    ]
                                },
                                "depends": "torch-ddp-create-torch-service"
                            },
                            {
                                "name": "torch-ddp-0-worker-1",
                                "template": "torch-ddp-1",
                                "arguments": {
                                    "parameters": [
                                        {
                                            "name": "n_iter",
                                            "value": "{{workflow.parameters.n_iter}}"
                                        },
                                        {
                                            "name": "n_seconds_sleep",
                                            "value": "{{workflow.parameters.n_seconds_sleep}}"
                                        }
                                    ]
                                },
                                "depends": "torch-ddp-create-torch-service"
                            },
                            {
                                "name": "torch-ddp-0-worker-2",
                                "template": "torch-ddp-2",
                                "arguments": {
                                    "parameters": [
                                        {
                                            "name": "n_iter",
                                            "value": "{{workflow.parameters.n_iter}}"
                                        },
                                        {
                                            "name": "n_seconds_sleep",
                                            "value": "{{workflow.parameters.n_seconds_sleep}}"
                                        }
                                    ]
                                },
                                "depends": "torch-ddp-create-torch-service"
                            },
                            {
                                "name": "torch-ddp-0-worker-3",
                                "template": "torch-ddp-3",
                                "arguments": {
                                    "parameters": [
                                        {
                                            "name": "n_iter",
                                            "value": "{{workflow.parameters.n_iter}}"
                                        },
                                        {
                                            "name": "n_seconds_sleep",
                                            "value": "{{workflow.parameters.n_seconds_sleep}}"
                                        }
                                    ]
                                },
                                "depends": "torch-ddp-create-torch-service"
                            },
                            {
                                "name": "torch-ddp-delete-torch-service",
                                "template": "torch-ddp-delete-torch-service",
                                "arguments": {},
                                "depends": "torch-ddp-0"
                            },
                            {
                                "name": "show-duration-param-0",
                                "template": "show-duration-param",
                                "arguments": {
                                    "parameters": [
                                        {
                                            "name": "a",
                                            "value": "{{tasks.torch-ddp-0.outputs.parameters.duration}}"
                                        }
                                    ]
                                },
                                "depends": "torch-ddp-0"
                            }
                        ]
                    }
                },
                {
                    "name": "torch-ddp-0",
                    "inputs": {
                        "parameters": [
                            {
                                "name": "n_iter",
                                "default": "100"
                            },
                            {
                                "name": "n_seconds_sleep",
                                "default": "10"
                            },
                            {
                                "name": "duration",
                                "default": "null"
                            }
                        ]
                    },
                    "outputs": {
                        "parameters": [
                            {
                                "name": "duration",
                                "value_from": {
                                    "path": "duration"
                                }
                            }
                        ]
                    },
                    "metadata": {
                        "labels": {
                            "torch-job": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6",
                            "torch-node": "0"
                        }
                    },
                    "script": {
                        "image": "bettmensch88/bettmensch.ai-torch:3.11-latest",
                        "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: n_iter = json.loads(r'''{{inputs.parameters.n_iter}}''')\nexcept: n_iter = r'''{{inputs.parameters.n_iter}}'''\ntry: n_seconds_sleep = json.loads(r'''{{inputs.parameters.n_seconds_sleep}}''')\nexcept: n_seconds_sleep = r'''{{inputs.parameters.n_seconds_sleep}}'''\n\nfrom bettmensch_ai.io import InputParameter\n\nfrom bettmensch_ai.io import OutputParameter\nduration = OutputParameter(\"duration\")\n\ndef torch_ddp(n_iter: InputParameter=100, n_seconds_sleep: InputParameter=10, duration: OutputParameter=None) -> None:\n    \"\"\"When decorated with the torch_component decorator, implements a\n    bettmensch_ai.TorchComponent that runs a torch DDP across pods and nodes in\n    your K8s cluster.\"\"\"\n    import time\n    from datetime import datetime as dt\n    import torch\n    import torch.distributed as dist\n    has_gpu = torch.cuda.is_available()\n    print(f'GPU present: {has_gpu}')\n    if has_gpu:\n        dist.init_process_group(backend='nccl')\n    else:\n        dist.init_process_group(backend='gloo')\n    for i in range(1, n_iter + 1):\n        time.sleep(n_seconds_sleep)\n        a = torch.tensor([dist.get_rank()])\n        print(f'{i}/{n_iter}: @{dt.now()}')\n        print(f'{i}/{n_iter}: Backend {dist.get_backend()}')\n        print(f'{i}/{n_iter}: World size {dist.get_world_size()}')\n        print(f'{i}/{n_iter}: Rank {dist.get_rank()}')\n        print(f'{i}/{n_iter}: This makes me worker process {dist.get_rank() + 1}/{dist.get_world_size()} globally!')\n        if has_gpu:\n            device = torch.device('cuda:0')\n            device_count = torch.cuda.device_count()\n            print(f'{i}/{n_iter}: GPU count: {device_count}')\n            device_name = torch.cuda.get_device_name(0)\n            print(f'{i}/{n_iter}: GPU name: {device_name}')\n            device_property = torch.cuda.get_device_capability(device)\n            print(f'{i}/{n_iter}: GPU property: {device_property}')\n        else:\n            device = torch.device('cpu')\n        a_placed = a.to(device)\n        print(f'{i}/{n_iter}: Pre-`all_reduce` tensor: {a_placed}')\n        dist.all_reduce(a_placed)\n        print(f'{i}/{n_iter}: Post-`all_reduce` tensor: {a_placed}')\n        print('===================================================')\n    if duration is not None:\n        duration_seconds = n_iter * n_seconds_sleep\n        duration.assign(duration_seconds)\n\nfrom bettmensch_ai.components import torch_distribute\n\ntorch_distribute_decorator=torch_distribute()\ntorch_distributed_function=torch_distribute_decorator(torch_ddp)\n\ntorch_distributed_function(n_iter,n_seconds_sleep,duration)",
                        "name": "",
                        "command": [
                            "python"
                        ],
                        "ports": [
                            {
                                "container_port": 29200,
                                "name": "ddp",
                                "protocol": "TCP"
                            }
                        ],
                        "env": [
                            {
                                "name": "NCCL_DEBUG",
                                "value": "INFO"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_min_nodes",
                                "value": "4"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_max_nodes",
                                "value": "4"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_node_rank",
                                "value": "0"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_nproc_per_node",
                                "value": "1"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_max_restarts",
                                "value": "1"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_start_method",
                                "value": "fork"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_rdzv_backend",
                                "value": "static"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_url",
                                "value": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6.argo.svc.cluster.local"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_port",
                                "value": "29200"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_run_id",
                                "value": "1"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_tee",
                                "value": "0"
                            }
                        ],
                        "resources": {
                            "limits": {
                                "cpu": "100m",
                                "memory": "700Mi",
                                "nvidia.com/gpu": "1"
                            },
                            "requests": {
                                "cpu": "100m",
                                "memory": "700Mi",
                                "nvidia.com/gpu": "1"
                            }
                        },
                        "image_pull_policy": "Always"
                    },
                    "retry_strategy": {
                        "limit": "1",
                        "retry_policy": "OnError"
                    },
                    "tolerations": [
                        {
                            "key": "nvidia.com/gpu",
                            "operator": "Exists",
                            "effect": "NoSchedule"
                        }
                    ],
                    "pod_spec_patch": "topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector:\n    matchExpressions:\n      - { key: torch-node, operator: In, values: ['0','1','2','3','4','5']}"
                },
                {
                    "name": "torch-ddp-1",
                    "inputs": {
                        "parameters": [
                            {
                                "name": "n_iter",
                                "default": "100"
                            },
                            {
                                "name": "n_seconds_sleep",
                                "default": "10"
                            },
                            {
                                "name": "duration",
                                "default": "null"
                            }
                        ]
                    },
                    "outputs": {
                        "parameters": [
                            {
                                "name": "duration",
                                "value_from": {
                                    "path": "duration"
                                }
                            }
                        ]
                    },
                    "metadata": {
                        "labels": {
                            "torch-job": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6",
                            "torch-node": "1"
                        }
                    },
                    "script": {
                        "image": "bettmensch88/bettmensch.ai-torch:3.11-latest",
                        "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: n_iter = json.loads(r'''{{inputs.parameters.n_iter}}''')\nexcept: n_iter = r'''{{inputs.parameters.n_iter}}'''\ntry: n_seconds_sleep = json.loads(r'''{{inputs.parameters.n_seconds_sleep}}''')\nexcept: n_seconds_sleep = r'''{{inputs.parameters.n_seconds_sleep}}'''\n\nfrom bettmensch_ai.io import InputParameter\n\nfrom bettmensch_ai.io import OutputParameter\nduration = OutputParameter(\"duration\")\n\ndef torch_ddp(n_iter: InputParameter=100, n_seconds_sleep: InputParameter=10, duration: OutputParameter=None) -> None:\n    \"\"\"When decorated with the torch_component decorator, implements a\n    bettmensch_ai.TorchComponent that runs a torch DDP across pods and nodes in\n    your K8s cluster.\"\"\"\n    import time\n    from datetime import datetime as dt\n    import torch\n    import torch.distributed as dist\n    has_gpu = torch.cuda.is_available()\n    print(f'GPU present: {has_gpu}')\n    if has_gpu:\n        dist.init_process_group(backend='nccl')\n    else:\n        dist.init_process_group(backend='gloo')\n    for i in range(1, n_iter + 1):\n        time.sleep(n_seconds_sleep)\n        a = torch.tensor([dist.get_rank()])\n        print(f'{i}/{n_iter}: @{dt.now()}')\n        print(f'{i}/{n_iter}: Backend {dist.get_backend()}')\n        print(f'{i}/{n_iter}: World size {dist.get_world_size()}')\n        print(f'{i}/{n_iter}: Rank {dist.get_rank()}')\n        print(f'{i}/{n_iter}: This makes me worker process {dist.get_rank() + 1}/{dist.get_world_size()} globally!')\n        if has_gpu:\n            device = torch.device('cuda:0')\n            device_count = torch.cuda.device_count()\n            print(f'{i}/{n_iter}: GPU count: {device_count}')\n            device_name = torch.cuda.get_device_name(0)\n            print(f'{i}/{n_iter}: GPU name: {device_name}')\n            device_property = torch.cuda.get_device_capability(device)\n            print(f'{i}/{n_iter}: GPU property: {device_property}')\n        else:\n            device = torch.device('cpu')\n        a_placed = a.to(device)\n        print(f'{i}/{n_iter}: Pre-`all_reduce` tensor: {a_placed}')\n        dist.all_reduce(a_placed)\n        print(f'{i}/{n_iter}: Post-`all_reduce` tensor: {a_placed}')\n        print('===================================================')\n    if duration is not None:\n        duration_seconds = n_iter * n_seconds_sleep\n        duration.assign(duration_seconds)\n\nfrom bettmensch_ai.components import torch_distribute\n\ntorch_distribute_decorator=torch_distribute()\ntorch_distributed_function=torch_distribute_decorator(torch_ddp)\n\ntorch_distributed_function(n_iter,n_seconds_sleep,duration)",
                        "name": "",
                        "command": [
                            "python"
                        ],
                        "env": [
                            {
                                "name": "NCCL_DEBUG",
                                "value": "INFO"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_min_nodes",
                                "value": "4"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_max_nodes",
                                "value": "4"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_node_rank",
                                "value": "1"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_nproc_per_node",
                                "value": "1"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_max_restarts",
                                "value": "1"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_start_method",
                                "value": "fork"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_rdzv_backend",
                                "value": "static"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_url",
                                "value": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6.argo.svc.cluster.local"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_port",
                                "value": "29200"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_run_id",
                                "value": "1"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_tee",
                                "value": "0"
                            }
                        ],
                        "resources": {
                            "limits": {
                                "cpu": "100m",
                                "memory": "700Mi",
                                "nvidia.com/gpu": "1"
                            },
                            "requests": {
                                "cpu": "100m",
                                "memory": "700Mi",
                                "nvidia.com/gpu": "1"
                            }
                        },
                        "image_pull_policy": "Always"
                    },
                    "retry_strategy": {
                        "limit": "1",
                        "retry_policy": "OnError"
                    },
                    "tolerations": [
                        {
                            "key": "nvidia.com/gpu",
                            "operator": "Exists",
                            "effect": "NoSchedule"
                        }
                    ],
                    "pod_spec_patch": "topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector:\n    matchExpressions:\n      - { key: torch-node, operator: In, values: ['0','1','2','3','4','5']}"
                },
                {
                    "name": "torch-ddp-2",
                    "inputs": {
                        "parameters": [
                            {
                                "name": "n_iter",
                                "default": "100"
                            },
                            {
                                "name": "n_seconds_sleep",
                                "default": "10"
                            },
                            {
                                "name": "duration",
                                "default": "null"
                            }
                        ]
                    },
                    "outputs": {
                        "parameters": [
                            {
                                "name": "duration",
                                "value_from": {
                                    "path": "duration"
                                }
                            }
                        ]
                    },
                    "metadata": {
                        "labels": {
                            "torch-job": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6",
                            "torch-node": "2"
                        }
                    },
                    "script": {
                        "image": "bettmensch88/bettmensch.ai-torch:3.11-latest",
                        "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: n_iter = json.loads(r'''{{inputs.parameters.n_iter}}''')\nexcept: n_iter = r'''{{inputs.parameters.n_iter}}'''\ntry: n_seconds_sleep = json.loads(r'''{{inputs.parameters.n_seconds_sleep}}''')\nexcept: n_seconds_sleep = r'''{{inputs.parameters.n_seconds_sleep}}'''\n\nfrom bettmensch_ai.io import InputParameter\n\nfrom bettmensch_ai.io import OutputParameter\nduration = OutputParameter(\"duration\")\n\ndef torch_ddp(n_iter: InputParameter=100, n_seconds_sleep: InputParameter=10, duration: OutputParameter=None) -> None:\n    \"\"\"When decorated with the torch_component decorator, implements a\n    bettmensch_ai.TorchComponent that runs a torch DDP across pods and nodes in\n    your K8s cluster.\"\"\"\n    import time\n    from datetime import datetime as dt\n    import torch\n    import torch.distributed as dist\n    has_gpu = torch.cuda.is_available()\n    print(f'GPU present: {has_gpu}')\n    if has_gpu:\n        dist.init_process_group(backend='nccl')\n    else:\n        dist.init_process_group(backend='gloo')\n    for i in range(1, n_iter + 1):\n        time.sleep(n_seconds_sleep)\n        a = torch.tensor([dist.get_rank()])\n        print(f'{i}/{n_iter}: @{dt.now()}')\n        print(f'{i}/{n_iter}: Backend {dist.get_backend()}')\n        print(f'{i}/{n_iter}: World size {dist.get_world_size()}')\n        print(f'{i}/{n_iter}: Rank {dist.get_rank()}')\n        print(f'{i}/{n_iter}: This makes me worker process {dist.get_rank() + 1}/{dist.get_world_size()} globally!')\n        if has_gpu:\n            device = torch.device('cuda:0')\n            device_count = torch.cuda.device_count()\n            print(f'{i}/{n_iter}: GPU count: {device_count}')\n            device_name = torch.cuda.get_device_name(0)\n            print(f'{i}/{n_iter}: GPU name: {device_name}')\n            device_property = torch.cuda.get_device_capability(device)\n            print(f'{i}/{n_iter}: GPU property: {device_property}')\n        else:\n            device = torch.device('cpu')\n        a_placed = a.to(device)\n        print(f'{i}/{n_iter}: Pre-`all_reduce` tensor: {a_placed}')\n        dist.all_reduce(a_placed)\n        print(f'{i}/{n_iter}: Post-`all_reduce` tensor: {a_placed}')\n        print('===================================================')\n    if duration is not None:\n        duration_seconds = n_iter * n_seconds_sleep\n        duration.assign(duration_seconds)\n\nfrom bettmensch_ai.components import torch_distribute\n\ntorch_distribute_decorator=torch_distribute()\ntorch_distributed_function=torch_distribute_decorator(torch_ddp)\n\ntorch_distributed_function(n_iter,n_seconds_sleep,duration)",
                        "name": "",
                        "command": [
                            "python"
                        ],
                        "env": [
                            {
                                "name": "NCCL_DEBUG",
                                "value": "INFO"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_min_nodes",
                                "value": "4"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_max_nodes",
                                "value": "4"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_node_rank",
                                "value": "2"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_nproc_per_node",
                                "value": "1"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_max_restarts",
                                "value": "1"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_start_method",
                                "value": "fork"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_rdzv_backend",
                                "value": "static"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_url",
                                "value": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6.argo.svc.cluster.local"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_port",
                                "value": "29200"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_run_id",
                                "value": "1"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_tee",
                                "value": "0"
                            }
                        ],
                        "resources": {
                            "limits": {
                                "cpu": "100m",
                                "memory": "700Mi",
                                "nvidia.com/gpu": "1"
                            },
                            "requests": {
                                "cpu": "100m",
                                "memory": "700Mi",
                                "nvidia.com/gpu": "1"
                            }
                        },
                        "image_pull_policy": "Always"
                    },
                    "retry_strategy": {
                        "limit": "1",
                        "retry_policy": "OnError"
                    },
                    "tolerations": [
                        {
                            "key": "nvidia.com/gpu",
                            "operator": "Exists",
                            "effect": "NoSchedule"
                        }
                    ],
                    "pod_spec_patch": "topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector:\n    matchExpressions:\n      - { key: torch-node, operator: In, values: ['0','1','2','3','4','5']}"
                },
                {
                    "name": "torch-ddp-3",
                    "inputs": {
                        "parameters": [
                            {
                                "name": "n_iter",
                                "default": "100"
                            },
                            {
                                "name": "n_seconds_sleep",
                                "default": "10"
                            },
                            {
                                "name": "duration",
                                "default": "null"
                            }
                        ]
                    },
                    "outputs": {
                        "parameters": [
                            {
                                "name": "duration",
                                "value_from": {
                                    "path": "duration"
                                }
                            }
                        ]
                    },
                    "metadata": {
                        "labels": {
                            "torch-job": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6",
                            "torch-node": "3"
                        }
                    },
                    "script": {
                        "image": "bettmensch88/bettmensch.ai-torch:3.11-latest",
                        "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: n_iter = json.loads(r'''{{inputs.parameters.n_iter}}''')\nexcept: n_iter = r'''{{inputs.parameters.n_iter}}'''\ntry: n_seconds_sleep = json.loads(r'''{{inputs.parameters.n_seconds_sleep}}''')\nexcept: n_seconds_sleep = r'''{{inputs.parameters.n_seconds_sleep}}'''\n\nfrom bettmensch_ai.io import InputParameter\n\nfrom bettmensch_ai.io import OutputParameter\nduration = OutputParameter(\"duration\")\n\ndef torch_ddp(n_iter: InputParameter=100, n_seconds_sleep: InputParameter=10, duration: OutputParameter=None) -> None:\n    \"\"\"When decorated with the torch_component decorator, implements a\n    bettmensch_ai.TorchComponent that runs a torch DDP across pods and nodes in\n    your K8s cluster.\"\"\"\n    import time\n    from datetime import datetime as dt\n    import torch\n    import torch.distributed as dist\n    has_gpu = torch.cuda.is_available()\n    print(f'GPU present: {has_gpu}')\n    if has_gpu:\n        dist.init_process_group(backend='nccl')\n    else:\n        dist.init_process_group(backend='gloo')\n    for i in range(1, n_iter + 1):\n        time.sleep(n_seconds_sleep)\n        a = torch.tensor([dist.get_rank()])\n        print(f'{i}/{n_iter}: @{dt.now()}')\n        print(f'{i}/{n_iter}: Backend {dist.get_backend()}')\n        print(f'{i}/{n_iter}: World size {dist.get_world_size()}')\n        print(f'{i}/{n_iter}: Rank {dist.get_rank()}')\n        print(f'{i}/{n_iter}: This makes me worker process {dist.get_rank() + 1}/{dist.get_world_size()} globally!')\n        if has_gpu:\n            device = torch.device('cuda:0')\n            device_count = torch.cuda.device_count()\n            print(f'{i}/{n_iter}: GPU count: {device_count}')\n            device_name = torch.cuda.get_device_name(0)\n            print(f'{i}/{n_iter}: GPU name: {device_name}')\n            device_property = torch.cuda.get_device_capability(device)\n            print(f'{i}/{n_iter}: GPU property: {device_property}')\n        else:\n            device = torch.device('cpu')\n        a_placed = a.to(device)\n        print(f'{i}/{n_iter}: Pre-`all_reduce` tensor: {a_placed}')\n        dist.all_reduce(a_placed)\n        print(f'{i}/{n_iter}: Post-`all_reduce` tensor: {a_placed}')\n        print('===================================================')\n    if duration is not None:\n        duration_seconds = n_iter * n_seconds_sleep\n        duration.assign(duration_seconds)\n\nfrom bettmensch_ai.components import torch_distribute\n\ntorch_distribute_decorator=torch_distribute()\ntorch_distributed_function=torch_distribute_decorator(torch_ddp)\n\ntorch_distributed_function(n_iter,n_seconds_sleep,duration)",
                        "name": "",
                        "command": [
                            "python"
                        ],
                        "env": [
                            {
                                "name": "NCCL_DEBUG",
                                "value": "INFO"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_min_nodes",
                                "value": "4"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_max_nodes",
                                "value": "4"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_node_rank",
                                "value": "3"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_nproc_per_node",
                                "value": "1"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_max_restarts",
                                "value": "1"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_start_method",
                                "value": "fork"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_rdzv_backend",
                                "value": "static"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_url",
                                "value": "torch-ddp-0-c3ee0689-7a0b-4be4-8754-a019d7030eb6.argo.svc.cluster.local"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_rdzv_endpoint_port",
                                "value": "29200"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_run_id",
                                "value": "1"
                            },
                            {
                                "name": "bettmensch_ai_distributed_torch_tee",
                                "value": "0"
                            }
                        ],
                        "resources": {
                            "limits": {
                                "cpu": "100m",
                                "memory": "700Mi",
                                "nvidia.com/gpu": "1"
                            },
                            "requests": {
                                "cpu": "100m",
                                "memory": "700Mi",
                                "nvidia.com/gpu": "1"
                            }
                        },
                        "image_pull_policy": "Always"
                    },
                    "retry_strategy": {
                        "limit": "1",
                        "retry_policy": "OnError"
                    },
                    "tolerations": [
                        {
                            "key": "nvidia.com/gpu",
                            "operator": "Exists",
                            "effect": "NoSchedule"
                        }
                    ],
                    "pod_spec_patch": "topologySpreadConstraints:\n- maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector:\n    matchExpressions:\n      - { key: torch-node, operator: In, values: ['0','1','2','3','4','5']}"
                },
                {
                    "name": "show-duration-param",
                    "inputs": {
                        "parameters": [
                            {
                                "name": "a"
                            }
                        ]
                    },
                    "outputs": {},
                    "metadata": {},
                    "script": {
                        "image": "bettmensch88/bettmensch.ai:3.11-latest",
                        "source": "import os\nimport sys\nsys.path.append(os.getcwd())\n\n# --- preprocessing\nimport json\ntry: a = json.loads(r'''{{inputs.parameters.a}}''')\nexcept: a = r'''{{inputs.parameters.a}}'''\n\nfrom bettmensch_ai.io import InputParameter\n\ndef show_parameter(a: InputParameter) -> None:\n    \"\"\"When decorated with the bettmensch_ai.components.component decorator,\n    implements a bettmensch_ai.Component that prints the values of its\n    InputParameter.\"\"\"\n    print(f'Content of input parameter a is: {a}')\nshow_parameter(a)",
                        "name": "",
                        "command": [
                            "python"
                        ],
                        "resources": {
                            "limits": {
                                "cpu": "100m",
                                "memory": "100Mi"
                            },
                            "requests": {
                                "cpu": "100m",
                                "memory": "100Mi"
                            }
                        },
                        "image_pull_policy": "Always"
                    },
                    "retry_strategy": {
                        "limit": "1",
                        "retry_policy": "OnError"
                    }
                }
            ],
            "entrypoint": "bettmensch-ai-dag",
            "arguments": {
                "parameters": [
                    {
                        "name": "n_iter",
                        "value": "12"
                    },
                    {
                        "name": "n_seconds_sleep",
                        "value": "5"
                    }
                ]
            },
            "service_account_name": "argo-workflow",
            "workflow_template_ref": {
                "name": "pipeline-test-torch-gpu-pipeline-dcfq8"
            }
        },
        "artifact_repository_ref": {
            "config_map": "artifact-repositories",
            "key": "bettmensch-ai-artifact-repository",
            "namespace": "argo",
            "artifact_repository": {
                "s3": {
                    "endpoint": "s3.us-east-2.amazonaws.com",
                    "bucket": "bettmensch-ai-artifact-repository",
                    "insecure": true
                }
            }
        },
        "artifact_gc_status": {
            "not_specified": true
        },
        "task_results_completion_status": {
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-1501533811": true,
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-1906221877": true,
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2258088662": true,
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2336401843": true,
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2953909358": true,
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-2966531784": true,
            "pipeline-test-torch-gpu-pipeline-dcfq8-flow-2tzsx-842282759": true
        }
    }
}